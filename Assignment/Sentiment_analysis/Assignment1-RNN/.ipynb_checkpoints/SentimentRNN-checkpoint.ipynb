{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sys\n",
    "import os\n",
    "import string, nltk\n",
    "#nltk.download('stopwords')\n",
    "nltk.data.path.append(\"/home/ubuntu/nltk_data\")\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import torch\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = []\n",
    "seqence_len = 50\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation)) \n",
    "\n",
    "def Norm(text,wordnet_lemmatizer,stop_words):\n",
    "    text = text.lower().replace(\"\\s+\",\" \")\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            w = wordnet_lemmatizer.lemmatize(w, pos=\"v\")\n",
    "            filtered_sentence.append(w) \n",
    "    texts=\" \".join(str(x) for x in filtered_sentence)\n",
    "    return text\n",
    "\n",
    "def pad_text(encoded_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "def LoadData(file, Vocab=Vocab):\n",
    "    with open(file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        contents = f.read().splitlines()\n",
    "        for line in contents:\n",
    "            try:\n",
    "                _,text,label = line.split(\"#\")\n",
    "            except:\n",
    "                continue\n",
    "            text = text.split(\" \",1)[1]\n",
    "            \n",
    "            text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "            text = Norm(text,wordnet_lemmatizer,stop_words)\n",
    "            \n",
    "            data_x.append(text)\n",
    "            data_y.append(label)\n",
    "            Vocab = Vocab + text.split(\" \")\n",
    "        return data_x, data_y, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x, train_y, Vocab = LoadData(\"../data/train.txt\",Vocab)\n",
    "dev_x, dev_y, Vocab = LoadData(\"../data/dev.txt\",Vocab)\n",
    "test_x, test_y, Vocab = LoadData(\"../data/test_origin.txt\",Vocab)\n",
    "\n",
    "\n",
    "word_counts = Counter(Vocab)\n",
    "\n",
    "word_list = sorted(word_counts, key = word_counts.get, reverse = True)\n",
    "vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)}\n",
    "int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n",
    "\n",
    "\n",
    "encoded_train = [[vocab_to_int[word] for word in review.split(\" \")] for review in train_x]\n",
    "train_x = pad_text(encoded_train, seq_length = seqence_len)\n",
    "train_y = np.array([1 if label == \"pos\" else 0 for label in train_y])\n",
    "\n",
    "encoded_dev = [[vocab_to_int[word] for word in review.split(\" \")] for review in dev_x]\n",
    "dev_x = pad_text(encoded_dev, seq_length = seqence_len)\n",
    "dev_y = np.array([1 if label == \"pos\" else 0 for label in dev_y])\n",
    "\n",
    "encoded_test = [[vocab_to_int[word] for word in review.split(\" \")] for review in test_x]\n",
    "test_x = pad_text(encoded_test, seq_length = seqence_len)\n",
    "test_y = np.array([1 if label == \"pos\" else 0 for label in test_y])\n",
    "\n",
    "\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class NetworkLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):                       # => batch size, sent len\n",
    "        embedded_words = self.embedding(input_words)    # => (batch_size, seq_length, n_embed)\n",
    "        lstm_out, hidden = self.lstm(embedded_words)         # =>  (batch_size, seq_length, n_hidden)\n",
    "        \n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "\n",
    "class NetworkLSTM_(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wir = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whr = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wif = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whf = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wig = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whg = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wio = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Who = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            ir=embedded_words[i].matmul(self.Wir)\n",
    "            hr=hidden.matmul(   self.Whr)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iff=embedded_words[i].matmul(self.Wif)\n",
    "            hff=hidden.matmul(   self.Whf)\n",
    "            ff= iff.add(hff)\n",
    "            fft = self.sigmoid(ff)\n",
    "            \n",
    "            ig=embedded_words[i].matmul(self.Wig)\n",
    "            hg=hidden.matmul(   self.Whg)\n",
    "            g= ig.add(hg)\n",
    "            gt = self.tanh(g)\n",
    "            \n",
    "            io=embedded_words[i].matmul(self.Wio)\n",
    "            ho=hidden.matmul(   self.Who)\n",
    "            o= io.add(ho)\n",
    "            ot = self.sigmoid(o)\n",
    "            \n",
    "            c = fft*c + rt*gt\n",
    "            hidden = ot*self.tanh(c)\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "class NetworkGRU_(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wir = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whr = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wiz = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whz = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Win = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whn = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            ir=embedded_words[i].matmul(self.Wir)\n",
    "            hr=hidden.matmul(   self.Whr)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            #print(rt.shape)\n",
    "            \n",
    "            iz=embedded_words[i].matmul(self.Wiz)\n",
    "            hz=hidden.matmul(   self.Whz)\n",
    "            z= iz.add(hz)\n",
    "            zt = self.sigmoid(z)\n",
    "            \n",
    "            iN=embedded_words[i].matmul(self.Win)\n",
    "            hN=hidden.matmul(   self.Whz)*rt\n",
    "            N= iN.add(hN)\n",
    "            Nt = self.tanh(N)\n",
    "            \n",
    "            hidden = (1-zt)*Nt + zt*hidden\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "class NetworkRNN_(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wi = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Wh = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "#         self.rnn = nn.RNN(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "#         self.rnn_cell = nn.RNNCell(n_embed, hidden_node)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            A=embedded_words[i].matmul(self.Wi)\n",
    "            B=hidden.matmul(   self.Wh)\n",
    "            C = A.add(B)\n",
    "            hidden = self.sigmoid(C)\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "class NetworkGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.gru = nn.GRU(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        gru_out, hidden = self.gru(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        \n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class NetworkRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.rnn = nn.RNN(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        gru_out, hidden = self.rnn(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        \n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2445,\n",
      "         944,   16,  231,  831,    8,    2, 4866, 1468,    1,  559,   72,  208,\n",
      "          11, 9463,    1,  195, 1941, 1022,   27, 1839,    4, 9464, 9465,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    3,   71, 1943, 5561,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,   37,   11, 2850,    6,  145,\n",
      "         266,   31,    1,   11,   79, 2633,    4,  695, 4336,    1,   13,   27,\n",
      "          15,  373,    5, 5341, 1839,    1,   11,    7,   43,   25,  214,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,    11,    28,     2,   682,     5,     3,\n",
      "          408,  3211,  4815,     1,     3, 10045, 10046,   282,  1557,    30,\n",
      "         4077,  2131,     4,  3510,   665,    14,  4354,     1,     1,     1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    3,  327,    4,   63,    1, 2882,  261, 1007,\n",
      "           1, 1605,  160,    5,    3, 2056,    8, 1361,    5,   18,  903,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,    1,\n",
      "           1,    1,  866,   21,    3,  275, 3169,    4, 2747,   11,  392,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,   18, 9733,  911,    5, 5948, 2637,    1,  228,  314,    4, 5855,\n",
      "           1,  513,  233,  760,    7,    3,  277,  966,   16,   34,  437,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,  111,   11,    3,\n",
      "         264, 1360,    8,   36,  589,   16,   81, 1897, 6260,    6,  145,    3,\n",
      "        1910,    6,  467,    1,    4,   10,    3, 1809,   29,   27,    9,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,    54,   116,     5,     3,   865, 10036,   709,\n",
      "           52,    20,   638,     6,  2518,     9,    15,    40,     7,    41,\n",
      "         1098,     1,     1,     1,    54,    16,   583,     1,     1,     1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           3, 3279, 7258,    9,  310,   26,   57,  988, 1018,  357, 2436,   14,\n",
      "         435,    3,  260,    5,  702,  480,    3,  254,  383,   16,  119,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     2,\n",
      "         1632,     7,  4004,   245,    16,   119,    38, 10210,     1,     4,\n",
      "          124,   620,    30,    53,     6,     2,  2703,     1,     1,     1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0, 2348,    1,   10,  246,  740,    1,   42,\n",
      "         213,   31,  120,   14, 2555, 9814, 9815,    1,   77,   42,  462,   93,\n",
      "         193,  290,    1,   13,   92,  739,    6, 2305,  401,    1,    1,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0, 4985,    2, 1622, 1621,    5,   49,  215,\n",
      "         524,    1, 9690,   45,    2,   40,   71,  161,    1, 1322,  710,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           12,     3,    51,   798,   481,    16,   233,     1,   440,     1,\n",
      "        10049,     5,     2, 10050,     7,     3,  1616,     1,     1,     1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    3,  533,    1,\n",
      "        1696, 1691,    5, 3810,    9,    1,    8,   66,  110, 2788,   64,    1,\n",
      "        1394,   53,    2,  649,  312,    5,   86,    8,    2,   97, 6199,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0, 9536,    7,    3,  141,    3, 3209, 9537,\n",
      "           9,   10,  173,    6,  182,   98, 1290, 1467,   27,   34,  752,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         444, 2044, 3215, 4483,  502, 2834,    4,  871,    6,  978, 6947,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,  310,   26,   18, 9451, 1637,    5, 1987,\n",
      "        2914,  877,    4,  164, 2086, 2014, 7204,    9,   30,  772,  199,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0, 10506,    36,    64,   108,     2,  7025,   229,\n",
      "            5,     3,  1247, 10507,  5274,     2,   203,     5,    36,  2462,\n",
      "            1,  2993,   982, 10508,   101,  6602,   616,     1,     1,     1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0, 4271,    1, 1274,  406,   56,    7,   18,\n",
      "         384,  134,   19, 2105,  148, 4271,    7,    3, 2129,  429,  302,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,   931, 10784,    42,   131,\n",
      "           35,     1,  1145,   182,    10,   484,  5425,     4, 10785,  3186,\n",
      "          154,     2,  6476,  5560,     5,   125,  1501, 10786,    26,  2504,\n",
      "         2215,    39,     2,  3203,  2533,    84,    56,     1,     1,     1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,  318, 1175,   30,   31, 1032,   33,   15,  402,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,  4981,  2686,  1047,     2,  4142, 10015,    16,    36, 10016,\n",
      "        10017,     8,     2,   449,     5,  1246, 10018,     1,     1,     1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,  130, 9877,    7,  850,    8,    3, 9878,  388,\n",
      "           5,   64,    1,   92,  118,   38, 1446,   39,  262,   39, 9879, 1509,\n",
      "          25,   36,  589,   12,    6, 3150,  114,  412,    5,  230,  785,    1,\n",
      "           1,    1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    3,  545,\n",
      "         160,   32,   97, 4441,  282, 5945,    1, 5539,  682,    7, 4428,   23,\n",
      "           3,  564,  277, 1585,    4,  458,  688,  104,  780,    2, 5136,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0, 1270,    5,  611,  381,   12,   37,   11,   93,\n",
      "          99,  127,   47,  462,    1,    4,    7,   89, 2171,    9,   64,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,    26,     3, 10848,  7159,    27,     3,\n",
      "         2278,  2603,     1,   360,  3811,     1,     7,     3,    73,    19,\n",
      "         5167,    21,  6909,     5,     3,    51,  1408,     1,     1,     1])\n",
      "tensor(0)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    2,  747,    5,    3, 2335,  541, 3266,\n",
      "           7,   29,    5,  114,  210,    1, 9763,  904,   39,  604,  267,    1,\n",
      "           1,    1])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_loader:\n",
    "    print(x[0])\n",
    "    print(y[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkLSTM(\n",
      "  (embedding): Embedding(19025, 300)\n",
      "  (lstm): LSTM(300, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f5f07c38fc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b0f768e05aaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_words)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                       \u001b[0;31m# => batch size, sent len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membedded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# => (batch_size, seq_length, n_embed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_words\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# =>  (batch_size, seq_length, n_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "\n",
    "n_vocab = len(vocab_to_int)\n",
    "n_embed = 300\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "layers = 1\n",
    "\n",
    "net = NetworkLSTM(n_vocab, n_embed, n_hidden, n_output, layers).cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "print(net)\n",
    "\n",
    "# inp = torch.zeros((1,200), dtype=torch.long) # [length, batch_size]\n",
    "# print(summary(net,(300) ))\n",
    "\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 8 #4\n",
    "clip = 5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "count = 0\n",
    "sums = 0 \n",
    "print(device)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        try:\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        except:\n",
    "            output[output < 0.0] = 0.0\n",
    "            output[output > 1.0] = 1.0\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                sums = sums + len(v_inputs)\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "                \n",
    "                output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "                #print(len(output))\n",
    "                ground = v_labels.detach().cpu().numpy().astype(int)\n",
    "                #print(len(ground))\n",
    "                \n",
    "                count = count + np.sum(output == ground)\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            print(count/len(test_x))\n",
    "            count = 0\n",
    "            print(sums)\n",
    "            print(len(test_x))\n",
    "            sums = 0\n",
    "            net.train()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "# torch.save(net.state_dict(), \"LSTM.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkLSTM(\n",
      "  (embedding): Embedding(20451, 300)\n",
      "  (lstm): LSTM(300, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cff61c9896e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mv_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mvalid_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-b0f768e05aaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_words)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                       \u001b[0;31m# => batch size, sent len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membedded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# => (batch_size, seq_length, n_embed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_words\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# =>  (batch_size, seq_length, n_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_vocab=len(vocab_to_int)\n",
    "n_embed=300\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "layers = 1\n",
    "\n",
    "net = NetworkLSTM(n_vocab, n_embed, n_hidden, n_output, layers).cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "print(net)\n",
    "\n",
    "# inp = torch.zeros((1,200), dtype=torch.long) # [length, batch_size]\n",
    "# print(summary(net,(300) ))\n",
    "\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 9\n",
    "clip = 5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "count = 0\n",
    "sums = 0 \n",
    "print(device)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        try:\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        except:\n",
    "            output[output < 0.0] = 0.0\n",
    "            output[output > 1.0] = 1.0\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                #print(v_inputs.shape)\n",
    "                sums = sums + len(v_inputs)\n",
    "                v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
    "\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "                \n",
    "                output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "                #print(len(output))\n",
    "                ground = v_labels.detach().cpu().numpy().astype(int)\n",
    "                #print(len(ground))\n",
    "                \n",
    "                count = count + np.sum(output == ground)\n",
    "                \n",
    "                \n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            \n",
    "            print(count/len(dev_x))\n",
    "            count = 0\n",
    "            print(sums)\n",
    "            sums = 0\n",
    "            \n",
    "            net.train()\n",
    "            \n",
    "torch.save(net.state_dict(), \"LSTM.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,   11,   30,    3,  739,   63,    5,  532,\n",
      "         322,   71,  994,    4, 3867,   14,   22,  123,  201,   10,  146,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
      "           1,    1,    1, 1002,    1,  356,    1,    4, 6839,   65,   15,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,   18, 3326,  288,   40,    5, 2391,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     2,   140,     5,\n",
      "           19,     9,   534,  2852,  4618,     4,     3,  1744, 10049,    13,\n",
      "           62,   705,   794,     5,    10, 19727,  3333,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,    11,   160,   482,     9,\n",
      "           18,   594, 20243,  2279,  1372,    25,     3,  7491,   544,    97,\n",
      "           20,    16,    52,     1,    13,    11,     7,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,  513, 4465,   45,    2, 2284,    5,   52,  755,  128,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,   197,  1921,  5835,     6,     2,\n",
      "        10069,     1,    11,   446,    53,   129,     1,    28,  5716, 19893,\n",
      "          460,   277,     1,    25,   251,    49,   119,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,   43,   46,   74,   78,  937,    1, 2133,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 293,   4, 923,\n",
      "          1,  13, 135, 388,  65,   1,   1,   1], device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,   26,    5,    2,\n",
      "          79,    5,    3, 1622, 2594,    5, 1314,   60,    1,    1,    1,    9,\n",
      "        3394,    9,   86,  759,  758,    9,   84, 4266,  235,    4,  234,   17,\n",
      "          10, 6140,    7,    3,  758,    9,    7,  693,    5,  226, 1512,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,    88,     7,\n",
      "            3, 19865,     1,    39,   101,    28,    11,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     2,  2648,     5,  4704,\n",
      "         4705,     7,     3,   267,   199,     5, 19779,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,    16,  5758,  4813,    15,   284,    23,    51,     3,\n",
      "         3373,     5,  1539,     1, 19688,     6,   911,    21,     2, 19689,\n",
      "            5,  1174,    12,    82, 19690,     8,  6572,     4, 19691,    21,\n",
      "            2, 19692,    31,    35,   107,  1869,  1188,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,   111, 19096, 19097,     6,     2,  3738,     5,   448,\n",
      "           54,    36,    18,  2802,    52,    57,     1,   163,  1276,     6,\n",
      "          190,   124,  1079,  2319,   313,    23,  1644,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "         122, 4362, 3054, 2054,    1,    2, 5878,    7, 1290,    4, 3766,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   17,\n",
      "           2,  669,    1,   26, 3767,  287,    1, 4063,   54,  212, 1156,   17,\n",
      "          26,   27, 5002,    5,   35,   62, 1628,    1,    3, 2219, 1173,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     2,    15,     7,\n",
      "            3,  3275, 19494,   139,   118,  2174,  1293,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,    38,  6404,    58,   554,    78,  1334,     6,\n",
      "            3,  3728, 19479,     5,  7076,     1,  7078,     4, 19480,  6647,\n",
      "            1,    11,     7,  1106,     3,    52,   158,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,   16,  824,   15,\n",
      "         314,  125,    2, 6802, 1968,    1,   23, 2127,   17,    2,  121,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,   11,  133,   20, 2387,\n",
      "           6,  223,   21,  310,   31,  331,    1,   13,    9, 2157, 2276, 5903,\n",
      "         408,   15, 6146,   32,  243,   48,    4, 4458,    2, 2954,  281,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,    18,  1826,     1, 10235,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,  121,   31, 3680,   22,    6, 2330,   33,    3,  727, 1748,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,    16,    19,     7,   101,     5,    18,  2508,\n",
      "          191,     1, 20174,     4,  6777,    10,   228,     8,     3, 10264,\n",
      "            5,   695,   446,     4,  5827,   475,    80,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   18,\n",
      "        4386,   15,    1,   43,   72,   22,  236,   22,   29,  263,  145,    5,\n",
      "          11,    1,  101,  932,    9, 1052,   22,   75,    7,   50,  145,    1,\n",
      "           1,    1], device='cuda:0')\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,   466,  2223,    30,  7466,    71,  2969,  2765,     4,  1284,\n",
      "           33,    86, 19528,  2232,    21,     2,  2084,     1,     1,     1],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2238e7c5b22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#print(len(inputs))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mv_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b0f768e05aaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_words)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                       \u001b[0;31m# => batch size, sent len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membedded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_words\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# => (batch_size, seq_length, n_embed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_words\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# =>  (batch_size, seq_length, n_hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/taco/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "#net.eval().to(device)\n",
    "count = 0\n",
    "sums = 0\n",
    "\n",
    "valid_losses = []\n",
    "\n",
    "for v_inputs, v_labels in test_loader:\n",
    "    sums = sums + len(v_inputs)\n",
    "    v1_inputs, v1_labels = v_inputs.to(device), v_labels.to(device)\n",
    "    \n",
    "    print(v1_inputs[9])\n",
    "    \n",
    "    #print(len(inputs))\n",
    "    \n",
    "    v_output, v_h = net(v1_inputs)\n",
    "    \n",
    "    \n",
    "#     v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "#     valid_losses.append(v_loss.item())\n",
    "        \n",
    "\n",
    "#     output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "#     #print(len(output))\n",
    "#     ground = v_labels.detach().cpu().numpy().astype(int)\n",
    "#     #print(len(ground))\n",
    "#     count = count + np.sum(output == ground)\n",
    "    \n",
    "print(count/len(test_x))\n",
    "print(len(test_x))\n",
    "print(sums)\n",
    "\n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(valid_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \n",
    "\n",
    "def inference(net, review, seq_length = seqence_len):\n",
    "    device = \"cuda\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    text = review.lower()\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    words = text\n",
    "    \n",
    "    encoded_words = [vocab_to_int[word] for word in words.split(\" \")]\n",
    "    padded_words = pad_text([encoded_words], seq_length)\n",
    "    padded_words = torch.from_numpy(padded_words).to(device)\n",
    "\n",
    "    \n",
    "    net.eval().to(device)\n",
    "    output, h = net(padded_words )#, h)\n",
    "    pred = torch.round(output.squeeze())  \n",
    "    return pred\n",
    "\n",
    "\n",
    "inference(net, \"I am sad\") \n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(checkpoint, test_file, Vocab):\n",
    "\n",
    "    #     n_vocab = len(vocab_to_int)\n",
    "#     n_embed = 400\n",
    "#     n_hidden = 512\n",
    "#     n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "#     layers = 1\n",
    "\n",
    "#     net = NetworkRNN(n_vocab, n_embed, n_hidden, n_output, layers).cuda()\n",
    "    \n",
    "#     test_x, test_y, Vocab = LoadData(\"../data/test.txt\",Vocab)\n",
    "\n",
    "#     net.load_state_dict(torch.load(\"LSTM.pt\"))\n",
    "    \n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    net.eval().to(device)\n",
    "    count = 0\n",
    "    for v_inputs, v_labels in test_loader:\n",
    "        v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        v_output, v_h = net(v_inputs)\n",
    "\n",
    "        output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "        #print(len(output))\n",
    "        ground = labels.detach().cpu().numpy().astype(int)\n",
    "        #print(len(ground))\n",
    "        count = count + np.sum(output == ground)\n",
    "    print(count/len(test_x))\n",
    "\n",
    "\n",
    "# In[162]:\n",
    "\n",
    "\n",
    "predict(\"LSTM.pt\",\"../data/test.txt\",Vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco",
   "language": "python",
   "name": "taco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
