{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sys\n",
    "import os\n",
    "import string, nltk\n",
    "#nltk.download('stopwords')\n",
    "nltk.data.path.append(\"/home/ubuntu/nltk_data\")\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import torch\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_csv('glove.6B.50d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = []\n",
    "seqence_len = 50\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation)) \n",
    "\n",
    "def create_embedding_matrix(word_index,embedding_dict,dimension):\n",
    "    embedding_matrix=np.zeros((len(word_index)+1,dimension))\n",
    "\n",
    "    for word,index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index]=embedding_dict[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def Norm(text,wordnet_lemmatizer,stop_words):\n",
    "    text = text.lower().strip()\n",
    "    text =  re.sub(' +', ' ', text)\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            w = wordnet_lemmatizer.lemmatize(w, pos=\"v\")\n",
    "            filtered_sentence.append(w) \n",
    "    texts=\" \".join(str(x) for x in filtered_sentence)\n",
    "    return text\n",
    "\n",
    "def pad_text(encoded_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "def LoadData(file, Vocab=Vocab):\n",
    "    with open(file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        contents = f.read().splitlines()\n",
    "        for line in contents:\n",
    "            try:\n",
    "                _,text,label = line.split(\"#\")\n",
    "            except:\n",
    "                continue\n",
    "            text = text.split(\" \",1)[1]\n",
    "            \n",
    "            text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "            text = Norm(text,wordnet_lemmatizer,stop_words)\n",
    "            \n",
    "            data_x.append(text)\n",
    "            data_y.append(label)\n",
    "            Vocab = Vocab + text.split(\" \")\n",
    "        return data_x, data_y, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['most of the storylines feel like time fillers between surf shots the movie isnt horrible but you can see mediocre cresting on the next wave', 'the whole movie is simply a lazy exercise in bad filmmaking that asks you to not only suspend your disbelief but your intelligence as well', 'drumline ably captures the complicated relationships in a marching band', 'its an interesting effort particularly for jfk conspiracy nuts and barrys coldfish act makes the experience worthwhile', 'its kind of sad that so many people put so much time and energy into this turkey', 'most haunting about fence is its conclusion when we hear the ultimate fate of these girls and realize much to our dismay that this really did happen noyces greatest mistake is thinking that we needed sweeping dramatic hollywood moments to keep us', 'what dumb and dumber would have been without the vulgarity and with an intelligent lifeaffirming script', 'allens best works understand why snobbery is a better satiric target than middleamerica diversions could ever be', 'as satisfyingly odd and intriguing a tale as it was a century and a half ago has a delightfully dour deadpan tone and stylistic consistency', 'an ambitiously naturalistic albeit halfbaked drama about an abused innercity autistic teen', 'barely goes beyond comic book status', 'instead of a witty expose on the banality and hypocrisy of too much kidvid we get an ugly meanspirited lashing out by an adult whos apparently been forced by his kids to watch too many barney videos', 'perhaps not since nelson eddy crooned his indian love call to jeanette macdonald has there been a movie so unabashedly canadian not afraid to risk american scorn or disinterest', 'a story we havent seen on the big screen before and its a story that we as americans and human beings should know', 'freundlichs made crudup a suburban architect and a cipher', 'witless and utterly pointless', 'its all stitched together with energy intelligence and verve enhanced by a surplus of vintage archive footage', 'what really surprises about wisegirls is its lowkey quality and genuine tenderness', 'an unsophisticated scifi drama that takes itself all too seriously', 'ultimately this is a frustrating patchwork an uneasy marriage of louis begleys source novel about schmidt and an old payne screenplay', 'as written by michael berg and michael j wilson from a story by wilson this relentless allwiseguysallthetime approach tries way too hard and gets tiring in no time at all', 'the piano teacher is not an easy film it forces you to watch people doing unpleasant things to each other and themselves and it maintains a cool distance from its material that is deliberately unsettling', 'roman polanskis autobiographical gesture at redemption is better than shindlers list it is more than merely a holocaust movie', 'a little objectivity could have gone a long way', 'bielinsky is a filmmaker of impressive talent', 'the scriptwriters are no less a menace to society than the films characters', 'an intelligent multilayered and profoundly humanist not to mention gently political meditation on the values of knowledge education and the affects of cultural and geographical displacement', 'no amount of good intentions is able to overcome the triviality of the story', 'resident evil is what comes from taking john carpenters ghosts of mars and eliminating the beheadings in other words about as bad a film youre likely to see all year', 'ive seen some bad singerturned actors but lil bow wow takes the cake', 'morton deserves an oscar nomination', 'though the films scenario is certainly not earthshaking this depiction of fluctuating female sexuality has two winning lead performances and charm to spare', 'marshall puts a suspenseful spin on standard horror flick formula', 'the only entertainment youll derive from this choppy and sloppy affair will be from unintentional giggles \\x96 several of them', 'a piece of mildly entertaining inoffensive fluff that drifts aimlessly for 90 minutes before lodging in the cracks of that evergrowing category unembarrassing but unmemorable', 'the directors many dodges and turns add up to little more than a screenful of gamesmanship thats low on both suspense and payoff', 'starproducer salma hayek and director julie taymor have infused frida with a visual style unique and inherent to the titular characters paintings and in the process created a masterful work of art of their own', 'what madonna does here cant properly be called acting more accurately its moving and its talking and its occasionally gesturing sometimes all at once', 'a painfully funny ode to bad behavior', 'mazel tov to a film about a familys joyous life acting on the yiddish stage', 'its a talking head documentary but a great one', 'fudges fact and fancy with such confidence that we feel as if were seeing something purer than the real thing', 'chicago is in many ways an admirable achievement', 'perhaps a better celebration of these unfairly dismissed heroes would be a film that isnt this painfully forced false and fabricated', 'maguire is a surprisingly effective peterspiderman', 'assayas ambitious sometimes beautiful adaptation of jacques chardonnes novel', 'the film equivalent of a toy chest whose contents get scattered over the course of 80 minutes', 'its the kind of movie you cant quite recommend because it is all windup and not much of a pitch yet you cant bring yourself to dislike it', 'painfully padded', 'after sitting through this sloppy madeformovie comedy special it makes me wonder if lawrence hates criticism so much that he refuses to evaluate his own work', 'this is a startling film that gives you a fascinating albeit depressing view of iranian rural life close to the iraqi border', 'this is a film about the irksome tiresome nature of complacency that remains utterly satisfied to remain the same throughout even as the hero of the story rediscovers his passion in life the mood remains oddly detached', 'the problem amazingly enough is the screenplay', 'in the directors cut the film is not only a love song to the movies but it also is more fully an example of the kind of lush allenveloping movie experience it rhapsodizes', 'the fourth pokemon is a divertingif predictableadventure suitable for a matinee with a message that cautions children about disturbing the worlds delicate ecological balance', 'half submarine flick half ghost story all in one criminally neglected film', 'be patient with the lovely hush and your reward will be a thoughtful emotional movie experience', 'it extends the writings of jean genet and john rechy the films of fassbinder perhaps even the nocturnal works of goya', 'jackson and co have brought back the value and respect for the term epic cinema', 'theres no denying the physically spectacular qualities of the film or the emotional integrity of the performances', 'hardly a masterpiece but it introduces viewers to a good charitable enterprise and some interesting real people', 'a little weak and it isnt that funny', 'no its not nearly as good as any of its influences', 'its just not very smart', 'a muddled limp biscuit of a movie a vampire soap opera that doesnt make much sense even on its own terms', 'schnitzler does a fine job contrasting the sleekness of the films present with the playful paranoia of the films past', 'the talents of the actors helps moonlight mile rise above its heartonitssleeve writing', 'at times however dogtown and zboys lapses into an insiders lingo and mindset that the uninitiated may find hard to follow or care about', 'pretend its a werewolf itself by avoiding eye contact and walking slowly away its fun but its a real howler', 'the best disney movie since the lion king', 'although mainstream american movies tend to exploit the familiar every once in a while a film arrives from the margin that gives viewers a chance to learn to grow to travel', 'janey forgets about her other obligations leading to a tragedy which is somehow guessable from the first few minutes maybe because it echoes the by now intolerable morbidity of so many recent movies', 'this movie has a strong message about never giving up on a loved one but its not an easy movie to watch and will probably disturb many who see it', 'uncommonly stylish but equally silly the picture fails to generate much suspense nor does it ask searching enough questions to justify its pretensions', 'this painfully unfunny farce traffics in tired stereotypes and encumbers itself with complications that have no bearing on the story', 'really does feel like a short stretched out to feature length', 'there is no pleasure in watching a child suffer just embarrassment and a vague sense of shame', 'ultimately the project comes across as clinical detached uninvolving possibly prompting audience members to wonder whats the point', 'has a plot full of twists upon knots and a nonstop parade of mocktarantino scuzbag types that starts out clever but veers into overkill', 'at its best which occurs often michael moores bowling for columbine rekindles the muckraking soulsearching spirit of the are we a sick society journalism of the 1960s', 'the whole thing comes off like a particularly amateurish episode of bewitched that takes place during spring break', 'its hampered by a lifetimechannel kind of plot and a lead actress who is out of her depth', 'the script feels as if it started to explore the obvious voyeuristic potential of hypertime but then backed off when the producers saw the grosses for spy kids', 'when the screenwriter responsible for one of the worst movies of one year directs an equally miserable film the following year youd have a hard time believing it was just coincidence', 'this movie sucks', 'catch me feels capable of charming the masses with star power a popinduced score and sentimental moments that have become a spielberg trademark', 'ever see one of those comedies that just seem like a bad idea from frame one', 'an afterschool special without the courage of its convictions', 'its an ambitious film and as with all ambitious films it has some problems but on the whole youre gonna like this movie', 'if the reallife story is genuinely inspirational the movie stirs us as well', 'your response to its new sequel analyze that may hinge on what you thought of the first film', 'even if you have no interest in the ganginfested eastvs west coast rap wars this modern mob music drama never fails to fascinate', 'its probably worth catching solely on its visual merits if only it had the story to match', 'too leisurely paced and visually drab for its own good it succeeds in being only sporadically amusing', 'some body will take you places you havent been and also places you have', 'gets bogged down by an overly sillified plot and stopandstart pacing', 'contrived as this may sound mr roses updating works surprisingly well', 'the movie isnt always easy to look at but if it is indeed a duty of art to reflect life than leigh has created a masterful piece of artistry right here', 'one welltimed explosion in a movie can be a knockout but a hundred of them can be numbing proof of this is ballistic ecks vs sever', 'this comic gem is as delightful as it is derivative']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, Vocab = LoadData(\"../data/train.txt\",Vocab)\n",
    "dev_x, dev_y, Vocab = LoadData(\"../data/dev.txt\",Vocab)\n",
    "test_x, test_y, Vocab = LoadData(\"../data/test.txt\",Vocab)\n",
    "\n",
    "# #\n",
    "# train_x = train_x + dev_x + test_x\n",
    "# train_y = train_y + dev_y + test_y\n",
    "# #\n",
    "print(train_x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(split=\" \")\n",
    "tokenizer.fit_on_texts(train_x+dev_x+test_x)\n",
    "\n",
    "encoded_train =tokenizer.texts_to_sequences(train_x)\n",
    "encoded_dev =tokenizer.texts_to_sequences(dev_x)\n",
    "encoded_test =tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "\n",
    "train_x = pad_text(encoded_train, seq_length = seqence_len)\n",
    "train_y = np.array([1 if label == \"pos\" else 0 for label in train_y])\n",
    "\n",
    "\n",
    "dev_x = pad_text(encoded_dev, seq_length = seqence_len)\n",
    "dev_y = np.array([1 if label == \"pos\" else 0 for label in dev_y])\n",
    "\n",
    "\n",
    "test_x = pad_text(encoded_test, seq_length = seqence_len)\n",
    "test_y = np.array([1 if label == \"pos\" else 0 for label in test_y])\n",
    "\n",
    "\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_matrix=create_embedding_matrix(tokenizer.word_index,embedding_dict=glove_embedding,dimension=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class NetworkLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.embedding.weight=torch.nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):                       # => batch size, sent len\n",
    "        embedded_words = self.embedding(input_words)    # => (batch_size, seq_length, n_embed)\n",
    "        lstm_out, hidden = self.lstm(embedded_words)         # =>  (batch_size, seq_length, n_hidden)\n",
    "        \n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "\n",
    "class NetworkLSTM_(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wir = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whr = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wif = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whf = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wig = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whg = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wio = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Who = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            ir=embedded_words[i].matmul(self.Wir)\n",
    "            hr=hidden.matmul(   self.Whr)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iff=embedded_words[i].matmul(self.Wif)\n",
    "            hff=hidden.matmul(   self.Whf)\n",
    "            ff= iff.add(hff)\n",
    "            fft = self.sigmoid(ff)\n",
    "            \n",
    "            ig=embedded_words[i].matmul(self.Wig)\n",
    "            hg=hidden.matmul(   self.Whg)\n",
    "            g= ig.add(hg)\n",
    "            gt = self.tanh(g)\n",
    "            \n",
    "            io=embedded_words[i].matmul(self.Wio)\n",
    "            ho=hidden.matmul(   self.Who)\n",
    "            o= io.add(ho)\n",
    "            ot = self.sigmoid(o)\n",
    "            \n",
    "            c = fft*c + rt*gt\n",
    "            hidden = ot*self.tanh(c)\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "class NetworkGRU_(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wir = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whr = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Wiz = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whz = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.Win = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Whn = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            ir=embedded_words[i].matmul(self.Wir)\n",
    "            hr=hidden.matmul(   self.Whr)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            #print(rt.shape)\n",
    "            \n",
    "            iz=embedded_words[i].matmul(self.Wiz)\n",
    "            hz=hidden.matmul(   self.Whz)\n",
    "            z= iz.add(hz)\n",
    "            zt = self.sigmoid(z)\n",
    "            \n",
    "            iN=embedded_words[i].matmul(self.Win)\n",
    "            hN=hidden.matmul(   self.Whz)*rt\n",
    "            N= iN.add(hN)\n",
    "            Nt = self.tanh(N)\n",
    "            \n",
    "            hidden = (1-zt)*Nt + zt*hidden\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "class NetworkRNN_(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wi = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "        self.Wh = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "#         self.rnn = nn.RNN(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "#         self.rnn_cell = nn.RNNCell(n_embed, hidden_node)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            A=embedded_words[i].matmul(self.Wi)\n",
    "            B=hidden.matmul(   self.Wh)\n",
    "            C = A.add(B)\n",
    "            hidden = self.sigmoid(C)\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "class NetworkGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.gru = nn.GRU(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        gru_out, hidden = self.gru(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        \n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class NetworkRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.rnn = nn.RNN(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        gru_out, hidden = self.rnn(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        \n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkRNN_(\n",
      "  (embedding): Embedding(20451, 50)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (relu): ReLU()\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/9 Step: 100 Training Loss: 0.6672 Validation Loss: 0.6672\n",
      "Epoch: 2/9 Step: 200 Training Loss: 0.6976 Validation Loss: 0.6976\n",
      "Epoch: 2/9 Step: 300 Training Loss: 0.7172 Validation Loss: 0.7172\n",
      "Epoch: 3/9 Step: 400 Training Loss: 0.7156 Validation Loss: 0.7156\n",
      "Epoch: 4/9 Step: 500 Training Loss: 0.6828 Validation Loss: 0.6828\n",
      "Epoch: 4/9 Step: 600 Training Loss: 0.7142 Validation Loss: 0.7142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_vocab=embedding_matrix.shape[0]\n",
    "n_embed=embedding_matrix.shape[1]\n",
    "n_hidden = 1024 #512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "layers = 1\n",
    "\n",
    "net = NetworkRNN_(n_vocab, n_embed, n_hidden, n_output, layers).cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "print(net)\n",
    "\n",
    "# inp = torch.zeros((1,200), dtype=torch.long) # [length, batch_size]\n",
    "# print(summary(net,(300) ))\n",
    "\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 9\n",
    "clip = 5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        try:\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        except:\n",
    "            output[output < 0.0] = 0.0\n",
    "            output[output > 1.0] = 1.0\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "            \n",
    "torch.save(net.state_dict(), \"LSTM.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval().to(device)\n",
    "count = 0\n",
    "sums = 0\n",
    "\n",
    "valid_losses = []\n",
    "\n",
    "for v_inputs, v_labels in test_loader:\n",
    "    sums = sums + len(v_inputs)\n",
    "    v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
    "    \n",
    "    #print(len(inputs))\n",
    "    \n",
    "    v_output, v_h = net(v_inputs)\n",
    "    \n",
    "    \n",
    "    v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "    valid_losses.append(v_loss.item())\n",
    "        \n",
    "\n",
    "    output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "    #print(len(output))\n",
    "    ground = v_labels.detach().cpu().numpy().astype(int)\n",
    "    #print(len(ground))\n",
    "    count = count + np.sum(output == ground)\n",
    "    \n",
    "print(count/len(test_x))\n",
    "print(len(test_x))\n",
    "print(sums)\n",
    "\n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(valid_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-93b8469973bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I am fun\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-93b8469973bc>\u001b[0m in \u001b[0;36minference\u001b[0;34m(net, review, seq_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mencoded_train\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpadded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpadded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# \n",
    "\n",
    "def inference(net, review, seq_length = 200):\n",
    "    device = \"cuda\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    text = review.lower()\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    words = text\n",
    "    \n",
    "    encoded_train =tokenizer.texts_to_sequences([words])\n",
    "    padded_words = pad_text(encoded_train, seq_length = 200)\n",
    "    padded_words = torch.from_numpy(padded_words).to(device)\n",
    "\n",
    "    \n",
    "    net.eval().to(device)\n",
    "    output, h = net(padded_words )#, h)\n",
    "    pred = torch.round(output.squeeze())  \n",
    "    return pred\n",
    "\n",
    "\n",
    "inference(net, \"I am fun\") \n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(checkpoint, test_file, Vocab):\n",
    "\n",
    "    #     n_vocab = len(vocab_to_int)\n",
    "#     n_embed = 400\n",
    "#     n_hidden = 512\n",
    "#     n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "#     layers = 1\n",
    "\n",
    "#     net = NetworkRNN(n_vocab, n_embed, n_hidden, n_output, layers).cuda()\n",
    "    \n",
    "#     test_x, test_y, Vocab = LoadData(\"../data/test.txt\",Vocab)\n",
    "\n",
    "#     net.load_state_dict(torch.load(\"LSTM.pt\"))\n",
    "    \n",
    "#     device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    net.eval().to(device)\n",
    "    count = 0\n",
    "    for v_inputs, v_labels in test_loader:\n",
    "        v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        v_output, v_h = net(v_inputs)\n",
    "\n",
    "        output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "        #print(len(output))\n",
    "        ground = labels.detach().cpu().numpy().astype(int)\n",
    "        #print(len(ground))\n",
    "        count = count + np.sum(output == ground)\n",
    "    print(count/len(test_x))\n",
    "\n",
    "\n",
    "# In[162]:\n",
    "\n",
    "\n",
    "predict(\"LSTM.pt\",\"../data/test.txt\",Vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco",
   "language": "python",
   "name": "taco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
