{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Cuda Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sys\n",
    "import os\n",
    "import string, nltk\n",
    "#nltk.download('stopwords')\n",
    "nltk.data.path.append(\"/home/ubuntu/nltk_data\")\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import torch\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "#from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Global variable and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqence_len = 40\n",
    "embed_len = 300\n",
    "Vocab = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation)) \n",
    "\n",
    "glove = pd.read_csv('glove.6B.'+str(embed_len)+'d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index,embedding_dict,dimension):\n",
    "    embedding_matrix=np.zeros((len(word_index)+1,dimension))\n",
    "\n",
    "    for word,index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index]=embedding_dict[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def Norm(text,wordnet_lemmatizer,stop_words):\n",
    "    text = text.lower().strip()\n",
    "    text =  re.sub(' +', ' ', text)\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            w = wordnet_lemmatizer.lemmatize(w, pos=\"v\")\n",
    "            filtered_sentence.append(w) \n",
    "    texts=\" \".join(str(x) for x in filtered_sentence)\n",
    "    return text\n",
    "\n",
    "def pad_text(encoded_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "def LoadData(file, Vocab=Vocab):\n",
    "    with open(file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        contents = f.read().splitlines()\n",
    "        for line in contents:\n",
    "            try:\n",
    "                _,text,label = line.split(\"#\")\n",
    "            except:\n",
    "                continue\n",
    "            text = text.split(\" \",1)[1]\n",
    "            \n",
    "            text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "            text = Norm(text,wordnet_lemmatizer,stop_words)\n",
    "            \n",
    "            data_x.append(text)\n",
    "            data_y.append(label)\n",
    "            Vocab = Vocab + text.split(\" \")\n",
    "        return data_x, data_y, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandra bullock and hugh grant make a great team but this predictable romantic comedy should get a pink slip', 'those eternally devoted to the insanity of black will have an intermittently good time feel free to go get popcorn whenever hes not onscreen', 'this is wild surreal stuff but brilliant and the camera just kind of sits there and lets you look at this and its like youre going from one room to the next and none of them have any relation to the other', 'this is a harrowing movie about how parents know where all the buttons are and how to push them', 'without shakespeares eloquent language the update is dreary and sluggish']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, Vocab = LoadData(\"../data/train.txt\",Vocab)\n",
    "dev_x, dev_y, Vocab = LoadData(\"../data/dev.txt\",Vocab)\n",
    "test_x, test_y, Vocab = LoadData(\"../data/test.txt\",Vocab)\n",
    "print(test_x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(split=\" \")\n",
    "tokenizer.fit_on_texts(train_x+dev_x+test_x)\n",
    "\n",
    "encoded_train =tokenizer.texts_to_sequences(train_x)\n",
    "encoded_dev =tokenizer.texts_to_sequences(dev_x)\n",
    "encoded_test =tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "\n",
    "train_x = pad_text(encoded_train, seq_length = seqence_len)\n",
    "train_y = np.array([1 if label == \"pos\" else 0 for label in train_y])\n",
    "\n",
    "\n",
    "dev_x = pad_text(encoded_dev, seq_length = seqence_len)\n",
    "dev_y = np.array([1 if label == \"pos\" else 0 for label in dev_y])\n",
    "\n",
    "\n",
    "test_x = pad_text(encoded_test, seq_length = seqence_len)\n",
    "test_y = np.array([1 if label == \"pos\" else 0 for label in test_y])\n",
    "\n",
    "# print(len(type(encoded_test)))\n",
    "\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_matrix=create_embedding_matrix(tokenizer.word_index,embedding_dict=glove_embedding,dimension=embed_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.rnn = nn.RNN(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        rnn_out, hidden = self.rnn(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        out = self.fc(rnn_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkRNN_reimplement(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "#         self.Wi = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Wh = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "#         self.rnn = nn.RNN(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "#         self.rnn_cell = nn.RNNCell(n_embed, hidden_node)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.linear_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            A = self.linear_hidden(hidden)\n",
    "            B = self.linear_input(embedded_words[i])\n",
    "            C =  A.add(B)\n",
    "            hidden = self.sigmoid(C)\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkGRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.gru = nn.GRU(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        gru_out, hidden = self.gru(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        out = self.fc(gru_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkGRU_reimplement(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.linear_hidden_r = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_hidden_z = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_hidden_n = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_r = nn.Linear(n_embed, n_hidden)\n",
    "        self.linear_input_z = nn.Linear(n_embed, n_hidden)\n",
    "        self.linear_input_n = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            ir=self.linear_input_r(embedded_words[i])\n",
    "            hr=self.linear_hidden_r(hidden)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iz=self.linear_input_z(embedded_words[i])\n",
    "            hz=self.linear_hidden_z(hidden)\n",
    "            z= iz.add(hz)\n",
    "            zt = self.sigmoid(z)\n",
    "            \n",
    "            \n",
    "            iN=self.linear_input_n(embedded_words[i])\n",
    "            hN=self.linear_hidden_n(hidden)*rt\n",
    "            N= iN.add(hN)\n",
    "            Nt = self.tanh(N)\n",
    "            \n",
    "            hidden = (1-zt)*Nt + zt*hidden\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.embedding.weight=torch.nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_embed, hidden_node, layers, batch_first = True, bidirectional=False)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):                       # => batch size, sent len\n",
    "        embedded_words = self.embedding(input_words)    # => (batch_size, seq_length, n_embed)\n",
    "        lstm_out, hidden = self.lstm(embedded_words)         # =>  (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLSTM_reimplement(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "        c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "        for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "            ir=self.linear_input(embedded_words[i])\n",
    "            hr=self.linear_hidden(hidden)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iff=self.linear_input(embedded_words[i])\n",
    "            hff=self.linear_hidden(hidden)\n",
    "            ff= iff.add(hff)\n",
    "            fft = self.sigmoid(ff)\n",
    "            \n",
    "            ig=self.linear_input(embedded_words[i])\n",
    "            hg=self.linear_hidden(hidden)\n",
    "            g= ig.add(hg)\n",
    "            gt = self.tanh(g)\n",
    "            \n",
    "            io=self.linear_input(embedded_words[i])\n",
    "            ho=self.linear_hidden(hidden)\n",
    "            o= io.add(ho)\n",
    "            ot = self.sigmoid(o)\n",
    "            \n",
    "            c = fft*c + rt*gt\n",
    "            hidden = ot*self.tanh(c)\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab=embedding_matrix.shape[0]\n",
    "n_embed=embedding_matrix.shape[1]\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkLSTM(\n",
      "  (embedding): Embedding(20451, 300)\n",
      "  (lstm): LSTM(300, 512, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "========================================================================================================\n",
      "Layer (type:depth-idx)                   Kernel Shape     Output Shape     Param #          Mult-Adds\n",
      "========================================================================================================\n",
      "├─Embedding: 1-1                         [300, 20451]     [1, 40, 300]     6,135,300        6,135,300\n",
      "├─LSTM: 1-2                              --               [1, 40, 512]     3,768,320        3,760,128\n",
      "|    └─weight_ih_l0                      [2048, 300]\n",
      "|    └─weight_hh_l0                      [2048, 512]\n",
      "|    └─weight_ih_l1                      [2048, 512]\n",
      "|    └─weight_hh_l1                      [2048, 512]\n",
      "├─Dropout: 1-3                           --               [1, 40, 512]     --               --\n",
      "├─Linear: 1-4                            [512, 1]         [1, 1]           513              512\n",
      "├─Sigmoid: 1-5                           --               [1, 1]           --               --\n",
      "========================================================================================================\n",
      "Total params: 9,904,133\n",
      "Trainable params: 9,904,133\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 9.90\n",
      "========================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.26\n",
      "Params size (MB): 39.62\n",
      "Estimated Total Size (MB): 39.88\n",
      "========================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================\n",
       "Layer (type:depth-idx)                   Kernel Shape     Output Shape     Param #          Mult-Adds\n",
       "========================================================================================================\n",
       "├─Embedding: 1-1                         [300, 20451]     [1, 40, 300]     6,135,300        6,135,300\n",
       "├─LSTM: 1-2                              --               [1, 40, 512]     3,768,320        3,760,128\n",
       "|    └─weight_ih_l0                      [2048, 300]\n",
       "|    └─weight_hh_l0                      [2048, 512]\n",
       "|    └─weight_ih_l1                      [2048, 512]\n",
       "|    └─weight_hh_l1                      [2048, 512]\n",
       "├─Dropout: 1-3                           --               [1, 40, 512]     --               --\n",
       "├─Linear: 1-4                            [512, 1]         [1, 1]           513              512\n",
       "├─Sigmoid: 1-5                           --               [1, 1]           --               --\n",
       "========================================================================================================\n",
       "Total params: 9,904,133\n",
       "Trainable params: 9,904,133\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 9.90\n",
       "========================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.26\n",
       "Params size (MB): 39.62\n",
       "Estimated Total Size (MB): 39.88\n",
       "========================================================================================================"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NetworkLSTM(n_vocab, n_embed, n_hidden, n_output, layers).cuda()\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "print(net)\n",
    "\n",
    "summary(\n",
    "    net,\n",
    "    (1, seqence_len),\n",
    "    dtypes=[torch.long],\n",
    "    verbose=2,\n",
    "    col_width=16,\n",
    "    col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 Step: 50 Training Loss: 0.6922\n",
      "Epoch: 1/5 Step: 100 Training Loss: 0.5348\n",
      "Epoch: 1/5 Step: 150 Training Loss: 0.4767\n",
      "Epoch: 2/5 Step: 200 Training Loss: 0.4820\n",
      "Epoch: 2/5 Step: 250 Training Loss: 0.2566\n",
      "Epoch: 2/5 Step: 300 Training Loss: 0.4884\n",
      "Epoch: 3/5 Step: 350 Training Loss: 0.3489\n",
      "Epoch: 3/5 Step: 400 Training Loss: 0.1567\n",
      "Epoch: 3/5 Step: 450 Training Loss: 0.0700\n",
      "Epoch: 4/5 Step: 500 Training Loss: 0.0279\n",
      "Epoch: 4/5 Step: 550 Training Loss: 0.1508\n",
      "Epoch: 4/5 Step: 600 Training Loss: 0.1399\n",
      "Epoch: 5/5 Step: 650 Training Loss: 0.0068\n",
      "Epoch: 5/5 Step: 700 Training Loss: 0.0568\n",
      "Epoch: 5/5 Step: 750 Training Loss: 0.0987\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "n_epochs = 5\n",
    "clip = 5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        try:\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        except:\n",
    "            output[output < 0.0] = 0.0\n",
    "            output[output > 1.0] = 1.0\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        #To prevent exploding gradients\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % 50) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
    "\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "                \n",
    "                \n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "            \n",
    "            net.train()\n",
    "            \n",
    "#torch.save(net.state_dict(), \"LSTM.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7709377236936292\n",
      "1397\n",
      "1397\n"
     ]
    }
   ],
   "source": [
    "net.eval().to(device)\n",
    "count = 0\n",
    "sums = 0\n",
    "\n",
    "for v_inputs, v_labels in test_loader:\n",
    "    \n",
    "    sums = sums + len(v_inputs)\n",
    "    \n",
    "    v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
    "\n",
    "    v_output, v_h = net(v_inputs)\n",
    "    \n",
    "    v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "        \n",
    "\n",
    "    output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "\n",
    "    ground = v_labels.detach().cpu().numpy().astype(int)\n",
    "\n",
    "    count = count + np.sum(output == ground)\n",
    "    \n",
    "print(\"Accuracy: \" + str(count/len(test_x)))\n",
    "print(len(test_x))\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postive:\tIt make me happy\n",
      "negative:\tUnpleasant viewing experience\n",
      "postive:\tI am interested with this assigment\n",
      "negative:\tPoor you\n",
      "postive:\tHappy new year\n"
     ]
    }
   ],
   "source": [
    "def inference(net, review, seq_length = 200):\n",
    "    device = \"cuda\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    text = review.lower()\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    words = text\n",
    "    \n",
    "    encoded_train =tokenizer.texts_to_sequences([words])\n",
    "    padded_words = pad_text(encoded_train, seq_length = 200)\n",
    "    padded_words = torch.from_numpy(padded_words).to(device)\n",
    "\n",
    "    \n",
    "    net.eval().to(device)\n",
    "    output, h = net(padded_words )#, h)\n",
    "    pred = torch.round(output.squeeze())  \n",
    "    return pred\n",
    "\n",
    "Test = [\n",
    "    \"It make me happy\",\n",
    "    \"Unpleasant viewing experience\",\n",
    "    \"I am interested with this assigment\",\n",
    "    \"Poor you\",\n",
    "    \"Happy new year\"\n",
    "]\n",
    "for t in Test:\n",
    "    lab = inference(net, t).tolist()\n",
    "    if int(lab) == 0:\n",
    "        print(\"negative:\\t\"+t)\n",
    "    else:\n",
    "        print(\"postive:\\t\"+t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NetworkLSTM_(nn.Module):\n",
    "    \n",
    "#     def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.Wir = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Whr = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "#         self.Wif = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Whf = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "#         self.Wig = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Whg = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "#         self.Wio = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Who = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "#         self.hidden_node = hidden_node\n",
    "#         self.layers = layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "#         self.fc = nn.Linear(n_hidden, n_output)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "#     def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "#         embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "#         embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "#         hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "#         c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "#         for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "#             ir=embedded_words[i].matmul(self.Wir)\n",
    "#             hr=hidden.matmul(   self.Whr)\n",
    "#             r= ir.add(hr)\n",
    "#             rt = self.sigmoid(r)\n",
    "            \n",
    "#             iff=embedded_words[i].matmul(self.Wif)\n",
    "#             hff=hidden.matmul(   self.Whf)\n",
    "#             ff= iff.add(hff)\n",
    "#             fft = self.sigmoid(ff)\n",
    "            \n",
    "#             ig=embedded_words[i].matmul(self.Wig)\n",
    "#             hg=hidden.matmul(   self.Whg)\n",
    "#             g= ig.add(hg)\n",
    "#             gt = self.tanh(g)\n",
    "            \n",
    "#             io=embedded_words[i].matmul(self.Wio)\n",
    "#             ho=hidden.matmul(   self.Who)\n",
    "#             o= io.add(ho)\n",
    "#             ot = self.sigmoid(o)\n",
    "            \n",
    "#             c = fft*c + rt*gt\n",
    "#             hidden = ot*self.tanh(c)\n",
    "        \n",
    "#         out = self.fc(hidden)\n",
    "        \n",
    "#         sig = self.sigmoid(out)\n",
    "#         return sig, hidden\n",
    "    \n",
    "# class NetworkGRU_(nn.Module):\n",
    "    \n",
    "#     def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.Wir = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Whr = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "#         self.Wiz = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Whz = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "#         self.Win = nn.Parameter(torch.randn( (n_embed,hidden_node), requires_grad=True, dtype=torch.float))\n",
    "#         self.Whn = nn.Parameter(torch.randn( (hidden_node,hidden_node) , requires_grad=True, dtype=torch.float))\n",
    "        \n",
    "#         self.hidden_node = hidden_node\n",
    "#         self.layers = layers\n",
    "        \n",
    "#         self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "#         self.fc = nn.Linear(n_hidden, n_output)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.tanh = nn.Tanh()\n",
    "#         self.dropout = nn.Dropout(0.6)\n",
    "        \n",
    "        \n",
    "#     def forward (self, input_words):                    # => (batch size, sent len)\n",
    "        \n",
    "#         embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "#         embedded_words = embedded_words.permute(1,0,2)   #  (seq_length,batch_size,  n_embed)\n",
    "#         hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)  # batch-node\n",
    "        \n",
    "#         for i in range(input_words.size(1)):           #for i in seq_length\n",
    "\n",
    "#             ir=embedded_words[i].matmul(self.Wir)\n",
    "#             hr=hidden.matmul(   self.Whr)\n",
    "#             r= ir.add(hr)\n",
    "#             rt = self.sigmoid(r)\n",
    "            \n",
    "#             #print(rt.shape)\n",
    "            \n",
    "#             iz=embedded_words[i].matmul(self.Wiz)\n",
    "#             hz=hidden.matmul(   self.Whz)\n",
    "#             z= iz.add(hz)\n",
    "#             zt = self.sigmoid(z)\n",
    "            \n",
    "#             iN=embedded_words[i].matmul(self.Win)\n",
    "#             hN=hidden.matmul(   self.Whz)*rt\n",
    "#             N= iN.add(hN)\n",
    "#             Nt = self.tanh(N)\n",
    "            \n",
    "#             hidden = (1-zt)*Nt + zt*hidden\n",
    "        \n",
    "#         out = self.fc(hidden)\n",
    "        \n",
    "#         sig = self.sigmoid(out)\n",
    "#         return sig, hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco",
   "language": "python",
   "name": "taco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
