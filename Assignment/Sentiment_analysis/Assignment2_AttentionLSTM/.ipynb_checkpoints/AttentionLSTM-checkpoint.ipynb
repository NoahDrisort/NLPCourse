{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Cuda Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sys\n",
    "import os\n",
    "import string, nltk\n",
    "#nltk.download('stopwords')\n",
    "nltk.data.path.append(\"/home/ubuntu/nltk_data\")\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import torch\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Global variable and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Pretrain Embedding from Glove, at this report, I use two pretrain glove with 100 and 300 dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqence_len = 150\n",
    "embed_len = 300\n",
    "batch_size = 512\n",
    "Vocab = []\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation)) \n",
    "\n",
    "glove = pd.read_csv(\"../../wordEmbedding/\"+'glove.6B.'+str(embed_len)+'d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = {key: val.values for key, val in glove.T.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Embedding matrix from pretrain\n",
    "- By using pretrain Glove, for each word, we map it to a representative vector\n",
    "\n",
    "Normalization the input text:\n",
    "- All  stopwords  and  punctuations  are  filter  out\n",
    "- With  lemmatization,  a  word  returns  an  actual  word  of  thelanguage. It reduces the inflected words properly ensuring thatthe  root  word  belongs  to  the  language\n",
    "\n",
    "Padding Text\n",
    "- To let all input sentences have the same length, so that we can train in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index,embedding_dict,dimension):\n",
    "    embedding_matrix=np.zeros((len(word_index)+1,dimension))\n",
    "\n",
    "    for word,index in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index]=embedding_dict[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def Norm(text,wordnet_lemmatizer,stop_words):\n",
    "    text = text.lower().strip()\n",
    "    text =  re.sub(' +', ' ', text)\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            w = wordnet_lemmatizer.lemmatize(w, pos=\"v\")\n",
    "            filtered_sentence.append(w) \n",
    "    texts=\" \".join(str(x) for x in filtered_sentence)\n",
    "    return text\n",
    "\n",
    "def pad_text(encoded_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "def LoadData(file, Vocab=Vocab):\n",
    "    with open(file, \"r\",encoding=\"ISO-8859-1\") as f:\n",
    "        data_x = []\n",
    "        data_y = []\n",
    "        contents = f.read().splitlines()\n",
    "        for line in contents:\n",
    "            try:\n",
    "                _,text,label = line.split(\"#\")\n",
    "            except:\n",
    "                continue\n",
    "            text = text.split(\" \",1)[1]\n",
    "            \n",
    "            text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "            text = Norm(text,wordnet_lemmatizer,stop_words)\n",
    "            \n",
    "            data_x.append(text)\n",
    "            data_y.append(label)\n",
    "            Vocab = Vocab + text.split(\" \")\n",
    "        return data_x, data_y, Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandra bullock and hugh grant make a great team but this predictable romantic comedy should get a pink slip', 'those eternally devoted to the insanity of black will have an intermittently good time feel free to go get popcorn whenever hes not onscreen', 'this is wild surreal stuff but brilliant and the camera just kind of sits there and lets you look at this and its like youre going from one room to the next and none of them have any relation to the other', 'this is a harrowing movie about how parents know where all the buttons are and how to push them', 'without shakespeares eloquent language the update is dreary and sluggish']\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, Vocab = LoadData(\"../data/train.txt\",Vocab)\n",
    "dev_x, dev_y, Vocab = LoadData(\"../data/dev.txt\",Vocab)\n",
    "test_x, test_y, Vocab = LoadData(\"../data/test.txt\",Vocab)\n",
    "print(test_x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lenght of sentences distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  1.,  0.,  0.,  0.,  3.,  2.,  0.,  3.,  2.,  1.,  2., 11.,\n",
       "        10., 13., 15., 11., 15., 12., 18., 19., 18., 24., 29., 20., 28.,\n",
       "        29., 27., 25., 23., 27., 29., 25., 16., 31., 24., 28., 27., 28.,\n",
       "        39., 33., 36., 29., 37., 38., 46., 42., 38., 41., 37., 48., 48.,\n",
       "        35., 56., 35., 49., 56., 38., 40., 43., 39., 47., 47., 56., 44.,\n",
       "        46., 50., 53., 39., 42., 44., 38., 57., 53., 64., 50., 57., 52.,\n",
       "        52., 53., 50., 65., 59., 58., 53., 64., 54., 58., 69., 64., 45.,\n",
       "        70., 74., 57., 49., 59., 53., 48., 54., 55., 73., 64., 68., 49.,\n",
       "        70., 52., 78., 55., 45., 45., 52., 61., 52., 56., 60., 65., 66.,\n",
       "        61., 55., 53., 65., 60., 51., 54., 49., 58., 54., 48., 58., 54.,\n",
       "        53., 37., 53., 36., 46., 53., 52., 41., 42., 45., 44., 52., 60.,\n",
       "        37., 53., 43., 38., 37., 43., 42., 45., 46., 35., 42., 38., 33.,\n",
       "        44., 34., 38., 39., 37., 36., 38., 39., 21., 33., 47., 34., 14.,\n",
       "        20., 23., 22., 34., 26., 19., 21., 22., 19., 22., 21., 20., 16.,\n",
       "        16., 22., 15., 12., 16., 17., 12., 16., 19., 19.,  8.,  8., 11.,\n",
       "        15., 17., 27.]),\n",
       " array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "         79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "         92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "        105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "        118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "        131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "        144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "        157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "        170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199]),\n",
       " <BarContainer object of 198 artists>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAATM0lEQVR4nO3dfYxl913f8fcHO+bBPNiOp6utnWU2xARZSHHckWuUELVZhzoJzS4lshyhsoClVSWgpAHB0kiFVv3D7gMhSKh0iVOWKiQOJtausAoxSwBVgoW1s4ntOK7XZg1erXeXxE5CqQCHb/+4Z8nd8Z2ZMzP36Tfzfkmje87vnjv3u+ee+ezv/M7DTVUhSWrPV826AEnSxhjgktQoA1ySGmWAS1KjDHBJatTl03yza6+9thYXF6f5lpLUvIcffvgvqmpheftUA3xxcZETJ05M8y0lqXlJnh3V3msIJcm/SfJ4kseSfDjJ1yTZneR4klNJ7ktyxXhLliStZs0AT3Id8K+Bpar6duAy4E7gHuB9VfUa4AXgrkkWKkm6VN+DmJcDX5vkcuDrgLPAm4H7u+cPA/vGXp0kaUVrBnhVnQH+C/BnDIL7C8DDwItV9VK32HPAdaNen+RAkhNJTly4cGE8VUuSeg2hXA3sBXYD/xC4Eri97xtU1aGqWqqqpYWFlx1ElSRtUJ8hlNuAP62qC1X1t8DHgDcAV3VDKgDXA2cmVKMkaYQ+Af5nwK1Jvi5JgD3AZ4BPAO/sltkPHJlMiZKkUfqMgR9ncLDyEeDR7jWHgJ8C3pPkFPBK4N4J1ilJWqbXhTxV9TPAzyxrfga4ZewVSZJ68V4o0jotHnyQxYMPzroMyQCXpFYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeDasrxiUludAS5JjTLAJalRBrg0xCEXtcQAl6RGGeCS1CgDXJIaZYBLUqPWDPAkr01ycujni0neneSaJA8leap7vHoaBUuSBvp8qfGTVXVTVd0E/CPgr4AHgIPAsaq6ATjWzUuSpmS9Qyh7gKer6llgL3C4az8M7BtjXZKkNaw3wO8EPtxN76iqs93088COUS9IciDJiSQnLly4sMEypfUZ9/ncnh+uedQ7wJNcAbwD+PXlz1VVATXqdVV1qKqWqmppYWFhw4VKki61nh74W4FHqupcN38uyU6A7vH8uIuTJK1sPQH+Lr4yfAJwFNjfTe8HjoyrKGk9vOugtqteAZ7kSuAtwMeGmu8G3pLkKeC2bl6SNCW9Aryq/m9VvbKqvjDU9rmq2lNVN1TVbVX1+cmVKU2fvXrNO6/ElKRGGeCS1CgDXJIaZYBLUqMMcElqlAEuTZHnrGucDHBJapQBrrm11Xqqa/17ttq/V5NngEtSowxwSWqUAa6pGscwwUq/Y1xDEB5oVCsMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktSovl+pdlWS+5N8NskTSb4jyTVJHkryVPd49aSLlSR9Rd8e+PuB36qqbwNeBzwBHASOVdUNwLFuXtr2+pxD7nnmGoc1AzzJNwFvAu4FqKq/qaoXgb3A4W6xw8C+yZQoSRqlTw98N3AB+B9JPpnkA9231O+oqrPdMs8DOyZVpCTp5S7vuczNwI9W1fEk72fZcElVVZIa9eIkB4ADALt27dpkuVJ71jNccnHZ03e/fVLlzPX7a3369MCfA56rquPd/P0MAv1ckp0A3eP5US+uqkNVtVRVSwsLC+OoWZJEjwCvqueBP0/y2q5pD/AZ4Ciwv2vbDxyZSIXa1sZ9YylvVKWtpM8QCsCPAh9KcgXwDPCDDML/o0nuAp4F7phMiZKkUXoFeFWdBJZGPLVnrNVIknrzSkw1YdL3AJ93o4Z+tsu/XSszwCWpUQa4Jm7WvWcPXGqrMsAlqVEGuCQ1ygDfZkYdCOs7vOAwxPq5zjRJBrgkNcoAl6RGGeASGztTZbPDI54do80ywCWpUQa4mjTO3uu89oL7XH1pL357M8AlqVEGuCQ1ygDXpvXZjR/HMlt12GQc/67h14/6XRv5/Q7PzD8DXJIaZYBLUqP6fiOPNBEt7aL3GQLqu+wsrPWFxfNYs1ZnD1ySGtWrB57kNPAl4MvAS1W1lOQa4D5gETgN3FFVL0ymTGl7sTesPtbTA/+nVXVTVV38bsyDwLGqugE41s1LkqZkM0Moe4HD3fRhYN+mq5Ek9db3IGYBH09SwH+vqkPAjqo62z3/PLBj1AuTHAAOAOzatWuT5WqebHQ3fz0HAyWtrG+Av7GqziT5B8BDST47/GRVVRfuL9OF/SGApaWlkctIktav1xBKVZ3pHs8DDwC3AOeS7AToHs9PqkhJ0sut2QNPciXwVVX1pW76u4D/ABwF9gN3d49HJlmoxm+7DVVs9HLyWZuHGjSf+gyh7AAeSHJx+V+rqt9K8ifAR5PcBTwL3DG5MiVJy60Z4FX1DPC6Ee2fA/ZMoiiNx3DPbaWr77azVnu2rdat8fNKTElqlAEuSY3yZlZalbvrmrW1bsK1ndkDl6RGGeCS1CiHUBo2y11Lh1baMK3PyWGO2bAHLkmNsgeuLWXaewat74ksHnxw073m1tdBy+yBS1KjDHBJapQBrnVZPPhgr11md6vb0fcz3ex7aPwMcElqlAEuSY0ywPUy7u5q2KjtwW1kPhjgktQozwPfAsZxLu+42DNrwzg+p5V+h1dlTo89cElqVO8AT3JZkk8m+c1ufneS40lOJbkvyRWTK1OStNx6euA/BjwxNH8P8L6qeg3wAnDXOAvTxjiEobW4jWwdvQI8yfXA24EPdPMB3gzc3y1yGNg3gfokSSvo2wP/eeAngb/r5l8JvFhVL3XzzwHXjbc0SdJq1jwLJcl3A+er6uEk/2S9b5DkAHAAYNeuXet9uabIXWupLX164G8A3pHkNPARBkMn7weuSnLxP4DrgTOjXlxVh6pqqaqWFhYWxlCyJAl6BHhV/XRVXV9Vi8CdwO9W1fcBnwDe2S22HzgysSq3oWncYGjc728PXn2ttn25HfW3mfPAfwp4T5JTDMbE7x1PSZKkPtZ1JWZV/R7we930M8At4y9JktSHl9I3avlu5mZ3Oyex2+qu8NaxkcvjV7rFg0Mn4+Ol9JLUKANckhrlEEpj/DozrWScn/u0hjm8c+Hm2AOXpEbZA2+AvRStV2t7Ya3VOy/sgUtSowxwSWqUQyjbxPAuqrurmhS3remyBy5JjTLAJalRBrikqdjsHTZXe+2s7945Kwa4JDXKg5hzbjv2KrR1zONN1rYSe+CS1CgDXJIaZYBLUqMMcElq1JoBnuRrkvxxkk8leTzJv+/adyc5nuRUkvuSXDH5creO7Xrak6Tx6dMD/2vgzVX1OuAm4PYktwL3AO+rqtcALwB3TaxKSdLLrBngNfCX3ewrup8C3gzc37UfBvZNokBJ0mi9xsCTXJbkJHAeeAh4Gnixql7qFnkOuG6F1x5IciLJiQsXLoyhZEkS9AzwqvpyVd0EXA/cAnxb3zeoqkNVtVRVSwsLCxurUpL0Mus6C6WqXgQ+AXwHcFWSi1dyXg+cGW9pkqTV9DkLZSHJVd301wJvAZ5gEOTv7BbbDxyZUI1zZ9QZJGudVTKOM048a0XSsD73QtkJHE5yGYPA/2hV/WaSzwAfSfIfgU8C906wTknSMmsGeFV9Gnj9iPZnGIyHb2vj7Fmfvvvt9rIl9eaVmJLUKANckhplgI/Zeg9urvZaSe2b5N+1AS5JjTLAJalRBrikLWO73eXTAJekRhngE7BSL2C79Q4kTZYBLkmNMsAlqVEG+DqNewjEIRVJG2WAS1KjDHBJapQBPkEOj0iT41ldBrgkNcsAl9S0tXrhw8+v9a1ZrfXoDXBJapQBLkmNWvMr1ZK8CvhVYAdQwKGqen+Sa4D7gEXgNHBHVb0wuVIlbReT/BLw4a8wbF2fHvhLwI9X1Y3ArcAPJ7kROAgcq6obgGPdvCRpStYM8Ko6W1WPdNNfAp4ArgP2Aoe7xQ4D+yZUoyRphHWNgSdZZPAN9ceBHVV1tnvqeQZDLKNecyDJiSQnLly4sJla505rR6ylrWKSQywt6R3gSb4e+A3g3VX1xeHnqqoYjI+/TFUdqqqlqlpaWFjYVLGSpK9Y8yAmQJJXMAjvD1XVx7rmc0l2VtXZJDuB85MqctYWDz64JQ54SNvFqC8Xn9Tvn2U2rNkDTxLgXuCJqvq5oaeOAvu76f3AkfGXJ0laSZ8e+BuAfwk8muRk1/ZvgbuBjya5C3gWuGMiFUqSRlozwKvqfwNZ4ek94y1nvvS9BFeSZsErMSWpUQa4JDWq11kokrRdbfTS+2kMu9oDl6RG2QOfAQ+ISvNhPfcS7/uaabIHLkmNMsAlqVEGOJd+lVKLX6skaXxaulGWAS5JjTLAJalRnoUiqRkOb17KHrgkNWrbB7j/o0tq1bYPcElqlQEuSY3yIOYKHFqRtq95v4T+InvgktSoPt+J+cEk55M8NtR2TZKHkjzVPV492TIlScv16YH/CnD7sraDwLGqugE41s03YbVL5edxF0lSO6Z9K441A7yq/gD4/LLmvcDhbvowsG+8ZUmS1rLRg5g7qupsN/08sGOlBZMcAA4A7Nq1a4NvN372tiW1btMHMauqgFrl+UNVtVRVSwsLC5t9O0lSZ6MBfi7JToDu8fz4SpIk9bHRAD8K7O+m9wNHxlOOJKmvPqcRfhj4Q+C1SZ5LchdwN/CWJE8Bt3XzkqQpWvMgZlW9a4Wn9oy5FknSOngpvSRtwizPaPNSeklqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1assE+PIbykz7y0Uladq2TIBL0nazpQJ8VK97eN4euaStZEsFuCRtJwa4JDVqUwGe5PYkTyY5leTguIpazUaGQTygKWkr2nCAJ7kM+EXgrcCNwLuS3DiuwiRJq9tMD/wW4FRVPVNVfwN8BNg7nrIkSWvZzJcaXwf8+dD8c8A/Xr5QkgPAgW72L5M8uYH3uhb4i7//nfesvvBaz4/RJXXNkXmtC+a3tnmtC+a3tnmtC+astqFM2mhd3zyqceLfSl9Vh4BDm/kdSU5U1dKYShob61q/ea1tXuuC+a1tXuuC+a1t3HVtZgjlDPCqofnruzZJ0hRsJsD/BLghye4kVwB3AkfHU5YkaS0bHkKpqpeS/Ajw28BlwAer6vGxVXapTQ3BTJB1rd+81javdcH81javdcH81jbWulJV4/x9kqQp8UpMSWqUAS5JjZrrAJ/Fpfor1PGqJJ9I8pkkjyf5sa79Z5OcSXKy+3nbjOo7neTRroYTXds1SR5K8lT3ePWUa3rt0Ho5meSLSd49q3WW5INJzid5bKht5DrKwC90292nk9w85br+c5LPdu/9QJKruvbFJP9vaN390qTqWqW2FT+/JD/drbMnk/yzKdd131BNp5Oc7Nqnts5WyYnJbWdVNZc/DA6MPg28GrgC+BRw44xq2Qnc3E1/A/B/GNw+4GeBn5iDdXUauHZZ238CDnbTB4F7ZvxZPs/gYoSZrDPgTcDNwGNrrSPgbcD/AgLcChyfcl3fBVzeTd8zVNfi8HIzWmcjP7/u7+FTwFcDu7u/3cumVdey5/8r8O+mvc5WyYmJbWfz3AOfm0v1q+psVT3STX8JeILBlajzbC9wuJs+DOybXSnsAZ6uqmdnVUBV/QHw+WXNK62jvcCv1sAfAVcl2Tmtuqrq41X1Ujf7RwyusZi6FdbZSvYCH6mqv66qPwVOMfgbnmpdSQLcAXx4Eu+9mlVyYmLb2TwH+KhL9WcemkkWgdcDx7umH+l2fz447WGKIQV8PMnDGdy6AGBHVZ3tpp8HdsymNGBwjcDwH9Q8rDNYeR3N07b3Qwx6aRftTvLJJL+f5DtnVNOoz29e1tl3Aueq6qmhtqmvs2U5MbHtbJ4DfO4k+XrgN4B3V9UXgf8GfAtwE3CWwa7bLLyxqm5mcGfIH07ypuEna7C/NpPzRTO4yOsdwK93TfOyzi4xy3W0kiTvBV4CPtQ1nQV2VdXrgfcAv5bkG6dc1lx+fkPexaWdhamvsxE58ffGvZ3Nc4DP1aX6SV7B4EP5UFV9DKCqzlXVl6vq74BfZkK7jGupqjPd43ngga6Ocxd3x7rH87OojcF/Ko9U1bmuxrlYZ52V1tHMt70kPwB8N/B93R893fDE57rphxmMM3/rNOta5fObh3V2OfAvgPsutk17nY3KCSa4nc1zgM/NpfrduNq9wBNV9XND7cPjVd8DPLb8tVOo7cok33BxmsEBsMcYrKv93WL7gSPTrq1zSY9oHtbZkJXW0VHg+7uzBG4FvjC0CzxxSW4HfhJ4R1X91VD7Qgb34SfJq4EbgGemVVf3vit9fkeBO5N8dZLdXW1/PM3agNuAz1bVcxcbprnOVsoJJrmdTePo7CaO6r6NwZHcp4H3zrCONzLY7fk0cLL7eRvwP4FHu/ajwM4Z1PZqBkf/PwU8fnE9Aa8EjgFPAb8DXDOD2q4EPgd801DbTNYZg/9EzgJ/y2Cs8a6V1hGDswJ+sdvuHgWWplzXKQZjoxe3tV/qlv3e7jM+CTwC/PMZrLMVPz/gvd06exJ46zTr6tp/BfhXy5ad2jpbJScmtp15Kb0kNWqeh1AkSaswwCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1Kj/j/K08cQOgQ06wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Len_Sentences = [len(x) for x in train_x]\n",
    "binz = [x for x in range(1,200)]\n",
    "plt.hist(Len_Sentences, bins=binz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data, Setup Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this experiment, the data is random load for each batch, do not apply strategy same lenght sentences for each batch\n",
    "\n",
    "Base on the distribution, the squence_lenght is set to average 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tf.keras.preprocessing.text.Tokenizer(split=\" \")\n",
    "tokenizer.fit_on_texts(train_x+dev_x+test_x)\n",
    "\n",
    "encoded_train =tokenizer.texts_to_sequences(train_x)\n",
    "encoded_dev =tokenizer.texts_to_sequences(dev_x)\n",
    "encoded_test =tokenizer.texts_to_sequences(test_x)\n",
    "\n",
    "\n",
    "train_x = pad_text(encoded_train, seq_length = seqence_len)\n",
    "train_y = np.array([1 if label == \"pos\" else 0 for label in train_y])\n",
    "\n",
    "\n",
    "dev_x = pad_text(encoded_dev, seq_length = seqence_len)\n",
    "dev_y = np.array([1 if label == \"pos\" else 0 for label in dev_y])\n",
    "\n",
    "\n",
    "test_x = pad_text(encoded_test, seq_length = seqence_len)\n",
    "test_y = np.array([1 if label == \"pos\" else 0 for label in test_y])\n",
    "\n",
    "# print(len(type(encoded_test)))\n",
    "\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(dev_x), torch.from_numpy(dev_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_matrix=create_embedding_matrix(tokenizer.word_index,embedding_dict=glove_embedding,dimension=embed_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention LSTM  -  Basic dot-product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM_Dot(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_hidden = hidden_node\n",
    "        self.hidden_node = hidden_node\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "        self.linear_hidden_r = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_r = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_f = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_f = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_g = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_g = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_o = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_o = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        \n",
    "        self.fc_out_attent = nn.Linear(n_hidden*2, n_output)\n",
    "        self.fc_out_lstm = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward (self, input_words):                                                        # => (batch size, sent len)\n",
    "        \n",
    "        hidden_stack = []\n",
    "        batch = input_words.shape[0]\n",
    "        seq_lenght= input_words.size(1)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)                                        # => (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)                                      # => (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)              # batch-node\n",
    "        \n",
    "        c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "        for i in range(seq_lenght):                                                         #for i in seq_length\n",
    "\n",
    "            ir=self.linear_input_r(embedded_words[i])\n",
    "            hr=self.linear_hidden_r(hidden)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iff=self.linear_input_f(embedded_words[i])\n",
    "            hff=self.linear_hidden_f(hidden)\n",
    "            ff= iff.add(hff)\n",
    "            fft = self.sigmoid(ff)\n",
    "            \n",
    "            ig=self.linear_input_g(embedded_words[i])\n",
    "            hg=self.linear_hidden_g(hidden)\n",
    "            g= ig.add(hg)\n",
    "            gt = self.tanh(g)\n",
    "            \n",
    "            io=self.linear_input_o(embedded_words[i])\n",
    "            ho=self.linear_hidden_o(hidden)\n",
    "            o= io.add(ho)\n",
    "            ot = self.sigmoid(o)\n",
    "            \n",
    "            c = fft*c + rt*gt\n",
    "            hidden = ot*self.tanh(c)\n",
    "            \n",
    "            hidden_stack.append(hidden)\n",
    "        \n",
    "        \n",
    "        final_hidden = self.fc_out_lstm(hidden)                                                      #[batch, hidden-node]\n",
    "        \n",
    "        outputs = torch.stack(hidden_stack).permute(1,0,2)                                           #[batch, seq-len, hidden-node]\n",
    "        \n",
    "        attention_score = torch.bmm(outputs,final_hidden.view(batch,self.hidden_node,1)).squeeze(2)  # [batch, seq-len, hidden-node] * [batch, hidden-node,1] => [batch, seq-len]\n",
    "        \n",
    "        attention_distribution = self.softmax(attention_score)\n",
    "        \n",
    "        attention_output = torch.bmm(\n",
    "            outputs.permute(0,2,1),                                                                  #[batch, hidden-node, seq-len]\n",
    "            attention_distribution.view(batch,seq_lenght,1)                                          #[batch, seq_len, 1]\n",
    "        ).squeeze(2)                                                                                 #[batch, hidden-node]\n",
    "        \n",
    "        \n",
    "        out_atten = torch.cat([final_hidden, attention_output],dim=1)                                #[batch, hidden-node*2]  \n",
    "        out = self.fc_out_attent(out_atten)                                                          #[batch, hidden-node]\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention LSTM - Multiplicative\n",
    "\n",
    "Suppose the encoder hidden layer is hidden_node (512)  and the decoder hidden layer is n_hidden_decode (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AttentionLSTM_Multiplicative(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, hidden_node_decode, n_output, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_hidden = hidden_node\n",
    "        n_hidden_decode = int(hidden_node_decode)\n",
    "        self.hidden_node = hidden_node\n",
    "        self.hidden_node_decode = int(hidden_node_decode)\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "        self.linear_hidden_r = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_r = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_f = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_f = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_g = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_g = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_o = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_o = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        self.W_attention = nn.Parameter(torch.zeros([n_hidden_decode, n_hidden], device=device))\n",
    "        self.b_attention = nn.Parameter(torch.zeros([n_hidden], device=device))\n",
    "        \n",
    "        \n",
    "        self.fc_out_attent = nn.Linear(n_hidden + n_hidden_decode, n_output)\n",
    "        self.fc_out_lstm = nn.Linear(n_hidden, n_hidden_decode)\n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward (self, input_words):                                                        # => (batch size, sent len)\n",
    "        \n",
    "        hidden_stack = []\n",
    "        batch = input_words.shape[0]\n",
    "        seq_lenght= input_words.size(1)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)                                        # => (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)                                      # => (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)              # batch-node\n",
    "        \n",
    "        c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "        for i in range(seq_lenght):                                                         #for i in seq_length\n",
    "\n",
    "            ir=self.linear_input_r(embedded_words[i])\n",
    "            hr=self.linear_hidden_r(hidden)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iff=self.linear_input_f(embedded_words[i])\n",
    "            hff=self.linear_hidden_f(hidden)\n",
    "            ff= iff.add(hff)\n",
    "            fft = self.sigmoid(ff)\n",
    "            \n",
    "            ig=self.linear_input_g(embedded_words[i])\n",
    "            hg=self.linear_hidden_g(hidden)\n",
    "            g= ig.add(hg)\n",
    "            gt = self.tanh(g)\n",
    "            \n",
    "            io=self.linear_input_o(embedded_words[i])\n",
    "            ho=self.linear_hidden_o(hidden)\n",
    "            o= io.add(ho)\n",
    "            ot = self.sigmoid(o)\n",
    "            \n",
    "            c = fft*c + rt*gt\n",
    "            hidden = ot*self.tanh(c)\n",
    "            \n",
    "            hidden_stack.append(hidden)\n",
    "        \n",
    "        \n",
    "        final_hidden = self.fc_out_lstm(hidden)                                                      #[batch, hidden-node] => [batch, hidden-node-decode]\n",
    "        \n",
    "        outputs = torch.stack(hidden_stack).permute(1,2,0)                                           #[batch, hidden-node, seq-len]\n",
    "        \n",
    "        \n",
    "        \n",
    "        attention_score = torch.mm(final_hidden,self.W_attention) + self.b_attention                 # [batch,hidden-node-decode]*[hidden-node-decode, hidden-node]  # => [batch, hidden-node]\n",
    "\n",
    "        attention_score = torch.bmm(attention_score.view(batch,1, self.hidden_node),outputs).squeeze(1)# batch, 1, hidden-node] *[batch, hidden-node, seq-len] => [batch, seq-len]\n",
    "        \n",
    "        \n",
    "        attention_distribution = self.softmax(attention_score)                                       # [batch, seq-len]\n",
    "        \n",
    "        attention_output = torch.bmm(\n",
    "            outputs,                                                                                 #[batch, hidden-node, seq-len]\n",
    "            attention_distribution.view(batch,seq_lenght,1)                                          #[batch, seq_len, 1]\n",
    "        ).squeeze(2)                                                                                 #[batch, hidden-node]\n",
    "        \n",
    "        \n",
    "        out_atten = torch.cat([final_hidden, attention_output],dim=1)                                #[batch, hidden-node*2]  \n",
    "        out = self.fc_out_attent(out_atten)                                                          #[batch, hidden-node]\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention LSTM - Additive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AttentionLSTM_Additive(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, hidden_node_decode, n_output, layers, v_attention_dimensionality):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_hidden = hidden_node\n",
    "        n_hidden_decode = int(hidden_node_decode)\n",
    "        self.hidden_node = hidden_node\n",
    "        self.hidden_node_decode = int(hidden_node_decode)\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "        self.linear_hidden_r = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_r = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_f = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_f = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_g = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_g = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_o = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_o = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # For Additive\n",
    "        self.linear_additive_h = nn.Linear(n_hidden, v_attention_dimensionality)\n",
    "        self.linear_additive_s = nn.Linear(n_hidden_decode, v_attention_dimensionality)\n",
    "        \n",
    "        self.linear_vdim = nn.Linear(v_attention_dimensionality,1)\n",
    "        \n",
    "        #self.W_Vdim = nn.Parameter(torch.zeros([v_attention_dimensionality], device=device))\n",
    "        \n",
    "        \n",
    "        self.fc_out_attent = nn.Linear(n_hidden + n_hidden_decode, n_output)\n",
    "        self.fc_out_lstm = nn.Linear(n_hidden, n_hidden_decode)\n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward (self, input_words):                                                        # => (batch size, sent len)\n",
    "        \n",
    "        hidden_stack = []\n",
    "        batch = input_words.shape[0]\n",
    "        seq_lenght= input_words.size(1)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)                                        # => (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)                                      # => (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)              # batch-node\n",
    "        \n",
    "        c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "        for i in range(seq_lenght):                                                         #for i in seq_length\n",
    "\n",
    "            ir=self.linear_input_r(embedded_words[i])\n",
    "            hr=self.linear_hidden_r(hidden)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iff=self.linear_input_f(embedded_words[i])\n",
    "            hff=self.linear_hidden_f(hidden)\n",
    "            ff= iff.add(hff)\n",
    "            fft = self.sigmoid(ff)\n",
    "            \n",
    "            ig=self.linear_input_g(embedded_words[i])\n",
    "            hg=self.linear_hidden_g(hidden)\n",
    "            g= ig.add(hg)\n",
    "            gt = self.tanh(g)\n",
    "            \n",
    "            io=self.linear_input_o(embedded_words[i])\n",
    "            ho=self.linear_hidden_o(hidden)\n",
    "            o= io.add(ho)\n",
    "            ot = self.sigmoid(o)\n",
    "            \n",
    "            c = fft*c + rt*gt\n",
    "            hidden = ot*self.tanh(c)\n",
    "            \n",
    "            hidden_stack.append(hidden)\n",
    "        \n",
    "        \n",
    "        final_hidden = self.fc_out_lstm(hidden)                                                      #[batch, hidden-node] => [batch, hidden-node-decode]\n",
    "        \n",
    "        outputs = torch.stack(hidden_stack).permute(1,0,2)                                           #[batch, seq-len, hidden-node]\n",
    "        \n",
    "        WH = self.linear_additive_h(outputs)                                                         # => [batch, seq-len, v_dim]\n",
    "\n",
    "        final_hidden_seq = final_hidden.repeat(1,seq_lenght)                                         #[batch, hidden-node-decode] => [batch, seq-len, hidden-node-decode]\n",
    "        final_hidden_seq = final_hidden_seq.view(-1,seq_lenght, self.hidden_node_decode)             # ? Right way ? \n",
    "        \n",
    "        \n",
    "        WS = self.linear_additive_s(final_hidden_seq)                                                # => [batch, seq-len, v_dim]\n",
    "        \n",
    "        attention_score = self.tanh(WH+WS)  # => [batch, seq-len, v_dim]\n",
    "        \n",
    "        #print(self.W_Vdim.view(v_attention_dimensionality,1).shape)\n",
    "        #attention_score = torch.mm(attention_score,self.W_Vdim.view(v_attention_dimensionality,1))  #[batch, seq-len]\n",
    "\n",
    "        attention_score = self.linear_vdim(attention_score).squeeze(2)                               # ? Right way ? [batch, seq-len]\n",
    "        \n",
    "        attention_distribution = self.softmax(attention_score)                                       # [batch, seq-len]\n",
    "        \n",
    "        attention_output = torch.bmm(\n",
    "            outputs.permute(0,2,1),                                                                                 #[batch, hidden-node, seq-len]\n",
    "            attention_distribution.view(batch,seq_lenght,1)                                          #[batch, seq_len, 1]\n",
    "        ).squeeze(2)                                                                                 #[batch, hidden-node]\n",
    "        \n",
    "        \n",
    "        out_atten = torch.cat([final_hidden, attention_output],dim=1)                                #[batch, hidden-node+ hidden-node-decoder]  \n",
    "        out = self.fc_out_attent(out_atten)                                                          #[batch, hidden-node]\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Bi-LSTM - Additive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AttentionBiLSTM_Additive(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, hidden_node, hidden_node_decode, n_output, layers, v_attention_dimensionality):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_hidden = hidden_node\n",
    "        n_hidden_decode = int(hidden_node_decode)\n",
    "        self.hidden_node = hidden_node\n",
    "        self.hidden_node_decode = int(hidden_node_decode)\n",
    "        self.layers = layers\n",
    "        \n",
    "        # LSTM1\n",
    "        self.linear_hidden_r = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_r = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_f = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_f = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_g = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_g = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_o = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_o = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "        # LSTM2\n",
    "        self.linear_hidden_r2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_r2 = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_f2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_f2 = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_g2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_g2 = nn.Linear(n_embed, n_hidden)\n",
    "\n",
    "        self.linear_hidden_o2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_input_o2 = nn.Linear(n_embed, n_hidden)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        # For Additive\n",
    "        self.linear_additive_h2 = nn.Linear(n_hidden*2, v_attention_dimensionality)\n",
    "        self.linear_additive_s = nn.Linear(n_hidden_decode, v_attention_dimensionality)\n",
    "        \n",
    "        self.linear_vdim = nn.Linear(v_attention_dimensionality,1)\n",
    "        \n",
    "        self.fc_out_attent = nn.Linear(n_hidden*2 + n_hidden_decode, n_output)\n",
    "        self.fc_out_attentA = nn.Linear(n_hidden*2 + n_hidden_decode, int((n_hidden*2 + n_hidden_decode)/2))\n",
    "        self.fc_out_attentB = nn.Linear(int((n_hidden*2 + n_hidden_decode)/2), n_output)\n",
    "        self.fc_out_lstm = nn.Linear(n_hidden, n_hidden_decode)\n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward (self, input_words):                                                        # => (batch size, sent len)\n",
    "        \n",
    "        hidden_stack = []\n",
    "        hidden_stack2 = []\n",
    "        batch = input_words.shape[0]\n",
    "        seq_lenght= input_words.size(1)\n",
    "        \n",
    "        embedded_words = self.embedding(input_words)                                        # => (batch_size, seq_length, n_embed)\n",
    "        embedded_words = embedded_words.permute(1,0,2)                                      # => (seq_length,batch_size,  n_embed)\n",
    "        hidden = torch.zeros(input_words.size(0), self.hidden_node).to(device)              # batch-node\n",
    "        \n",
    "        c = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        c2 = torch.zeros(input_words.size(0), self.hidden_node).to(device)\n",
    "        \n",
    "        for i in range(seq_lenght):                                                         #for i in seq_length\n",
    "\n",
    "            ir=self.linear_input_r(embedded_words[i])\n",
    "            hr=self.linear_hidden_r(hidden)\n",
    "            r= ir.add(hr)\n",
    "            rt = self.sigmoid(r)\n",
    "            \n",
    "            iff=self.linear_input_f(embedded_words[i])\n",
    "            hff=self.linear_hidden_f(hidden)\n",
    "            ff= iff.add(hff)\n",
    "            fft = self.sigmoid(ff)\n",
    "            \n",
    "            ig=self.linear_input_g(embedded_words[i])\n",
    "            hg=self.linear_hidden_g(hidden)\n",
    "            g= ig.add(hg)\n",
    "            gt = self.tanh(g)\n",
    "            \n",
    "            io=self.linear_input_o(embedded_words[i])\n",
    "            ho=self.linear_hidden_o(hidden)\n",
    "            o= io.add(ho)\n",
    "            ot = self.sigmoid(o)\n",
    "            \n",
    "            c = fft*c + rt*gt\n",
    "            hidden = ot*self.tanh(c)\n",
    "            \n",
    "            hidden_stack.append(hidden)\n",
    "            \n",
    "        for i in range(seq_lenght-1, -1, -1):                                                         #for i in seq_length\n",
    "\n",
    "            ir2=self.linear_input_r2(embedded_words[i])\n",
    "            hr2=self.linear_hidden_r2(hidden)\n",
    "            r2= ir2.add(hr2)\n",
    "            rt2 = self.sigmoid(r2)\n",
    "            \n",
    "            iff2=self.linear_input_f2(embedded_words[i])\n",
    "            hff2=self.linear_hidden_f2(hidden)\n",
    "            ff2= iff2.add(hff2)\n",
    "            fft2 = self.sigmoid(ff2)\n",
    "            \n",
    "            ig2=self.linear_input_g2(embedded_words[i])\n",
    "            hg2=self.linear_hidden_g2(hidden)\n",
    "            g2= ig2.add(hg2)\n",
    "            gt2 = self.tanh(g2)\n",
    "            \n",
    "            io2=self.linear_input_o2(embedded_words[i])\n",
    "            ho2=self.linear_hidden_o2(hidden)\n",
    "            o2= io2.add(ho2)\n",
    "            ot2 = self.sigmoid(o2)\n",
    "            \n",
    "            c2 = fft2*c2 + rt2*gt2\n",
    "            hidden2 = ot2*self.tanh(c2)\n",
    "            \n",
    "            hidden_stack2.insert(0,hidden2)\n",
    "        \n",
    "        \n",
    "        final_hidden = self.fc_out_lstm(hidden)                                                      #[batch, hidden-node] => [batch, hidden-node-decode]\n",
    "        final_hidden = self.dropout(final_hidden)\n",
    "        \n",
    "        \n",
    "        outputs1 = torch.stack(hidden_stack).permute(1,0,2)                                           #[batch, seq-len, hidden-node]\n",
    "        outputs2 = torch.stack(hidden_stack2).permute(1,0,2)                                           #[batch, seq-len, hidden-node]\n",
    "\n",
    "        outputs = torch.cat((outputs1,outputs2),2)                                                   #[batch, seq-len, hidden-node*2]\n",
    "        WH = self.linear_additive_h2(outputs)                                                         # => [batch, seq-len, v_dim]\n",
    "        WH = self.dropout(WH)\n",
    "        \n",
    "        final_hidden_seq = final_hidden.repeat(1,seq_lenght)                                         #[batch, hidden-node-decode] => [batch, seq-len, hidden-node-decode]\n",
    "        final_hidden_seq = final_hidden_seq.view(-1,seq_lenght, self.hidden_node_decode)             # ? Right way ? \n",
    "        \n",
    "        \n",
    "        WS = self.linear_additive_s(final_hidden_seq)                                                # => [batch, seq-len, v_dim]\n",
    "        WS = self.dropout(WS)\n",
    "        \n",
    "        attention_score = self.tanh(WH+WS)  # => [batch, seq-len, v_dim]\n",
    "\n",
    "        attention_score = self.linear_vdim(attention_score).squeeze(2)                               # ? Right way ? [batch, seq-len]\n",
    "        attention_score = self.dropout(attention_score)\n",
    "        \n",
    "        attention_distribution = self.softmax(attention_score)                                       # [batch, seq-len]\n",
    "        \n",
    "        attention_output = torch.bmm(\n",
    "            outputs.permute(0,2,1),                                                                                 #[batch, hidden-node, seq-len]\n",
    "            attention_distribution.view(batch,seq_lenght,1)                                          #[batch, seq_len, 1]\n",
    "        ).squeeze(2)                                                                                 #[batch, hidden-node]\n",
    "        \n",
    "        out_atten = torch.cat([final_hidden, attention_output],dim=1)                                #[batch, hidden-node*2+hidden-node-decoder]  \n",
    "        \n",
    "        #out = self.fc_out_attent(out_atten)                                                          #[batch, hidden-node]\n",
    "        \n",
    "#         print(out_atten.shape)\n",
    "        out = self.fc_out_attentA(out_atten)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc_out_attentB(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        sig = self.sigmoid(out)\n",
    "        return sig, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab=embedding_matrix.shape[0]\n",
    "n_embed=embedding_matrix.shape[1]\n",
    "n_hidden = 512\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = AttentionLSTM_Dot(n_vocab, n_embed, n_hidden, n_output, layers).cuda()\n",
    "\n",
    "# n_hidden_decode = 256\n",
    "# net = AttentionLSTM_Multiplicative(n_vocab, n_embed, n_hidden, n_hidden_decode, n_output, layers).cuda()\n",
    "\n",
    "# n_hidden_decode = 512\n",
    "# v_attention_dimensionality = 512\n",
    "# net = AttentionLSTM_Additive(n_vocab, n_embed, n_hidden, n_hidden_decode, n_output, layers, v_attention_dimensionality).cuda()\n",
    "\n",
    "\n",
    "n_hidden = 512\n",
    "n_hidden_decode = 1024\n",
    "v_attention_dimensionality = 512\n",
    "net = AttentionBiLSTM_Additive(n_vocab, n_embed, n_hidden, n_hidden_decode, n_output, layers, v_attention_dimensionality).cuda()\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.0001, weight_decay=0.0001)\n",
    "\n",
    "# print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:152: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25 Step: 10 Training Loss: 0.6911 Validation Loss: 0.6916\n",
      "Epoch: 2/25 Step: 20 Training Loss: 0.6980 Validation Loss: 0.6884\n",
      "Epoch: 2/25 Step: 30 Training Loss: 0.6857 Validation Loss: 0.6851\n",
      "Epoch: 3/25 Step: 40 Training Loss: 0.6795 Validation Loss: 0.6814\n",
      "Epoch: 4/25 Step: 50 Training Loss: 0.6686 Validation Loss: 0.6794\n",
      "Epoch: 4/25 Step: 60 Training Loss: 0.6698 Validation Loss: 0.6753\n",
      "Epoch: 5/25 Step: 70 Training Loss: 0.6468 Validation Loss: 0.6721\n",
      "Epoch: 5/25 Step: 80 Training Loss: 0.6545 Validation Loss: 0.6679\n",
      "Epoch: 6/25 Step: 90 Training Loss: 0.6391 Validation Loss: 0.6636\n",
      "Epoch: 7/25 Step: 100 Training Loss: 0.6267 Validation Loss: 0.6575\n",
      "Epoch: 7/25 Step: 110 Training Loss: 0.6053 Validation Loss: 0.6618\n",
      "Epoch: 8/25 Step: 120 Training Loss: 0.5996 Validation Loss: 0.6503\n",
      "Epoch: 9/25 Step: 130 Training Loss: 0.5610 Validation Loss: 0.6485\n",
      "Epoch: 9/25 Step: 140 Training Loss: 0.5456 Validation Loss: 0.6447\n",
      "Epoch: 10/25 Step: 150 Training Loss: 0.5164 Validation Loss: 0.6402\n",
      "Epoch: 10/25 Step: 160 Training Loss: 0.4923 Validation Loss: 0.6413\n",
      "Epoch: 11/25 Step: 170 Training Loss: 0.4537 Validation Loss: 0.6555\n",
      "Epoch: 12/25 Step: 180 Training Loss: 0.4346 Validation Loss: 0.6433\n",
      "Epoch: 13/25 Step: 190 Training Loss: 0.3297 Validation Loss: 0.6886\n",
      "Epoch: 14/25 Step: 200 Training Loss: 0.2766 Validation Loss: 0.7046\n",
      "Epoch: 15/25 Step: 210 Training Loss: 0.2302 Validation Loss: 0.7253\n",
      "Epoch: 16/25 Step: 220 Training Loss: 0.1944 Validation Loss: 0.7613\n",
      "Epoch: 17/25 Step: 230 Training Loss: 0.1579 Validation Loss: 0.8047\n",
      "Epoch: 18/25 Step: 240 Training Loss: 0.1126 Validation Loss: 0.8907\n",
      "Epoch: 19/25 Step: 250 Training Loss: 0.0808 Validation Loss: 0.9514\n",
      "Epoch: 20/25 Step: 260 Training Loss: 0.0636 Validation Loss: 1.0361\n",
      "Epoch: 21/25 Step: 270 Training Loss: 0.0331 Validation Loss: 1.0993\n",
      "Epoch: 22/25 Step: 280 Training Loss: 0.0345 Validation Loss: 1.1179\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "n_epochs = 20\n",
    "clip = 5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        #To prevent exploding gradients\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if loss.item() < 0.02:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        if (step % 10) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            num_val_batch =0 \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                num_val_batch += 1\n",
    "                v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
    "\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "               \n",
    "            valid_losses = sum(valid_losses)/len(valid_losses)\n",
    "                \n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(valid_losses),\n",
    "                 )\n",
    "\n",
    "            if valid_losses - loss.item() > 0.2:\n",
    "                break\n",
    "                \n",
    "            net.train()\n",
    "            \n",
    "#torch.save(net.state_dict(), \"LSTM.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:152: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6728704366499642\n",
      "1397\n",
      "1397\n"
     ]
    }
   ],
   "source": [
    "net.eval().to(device)\n",
    "count = 0\n",
    "sums = 0\n",
    "\n",
    "for v_inputs, v_labels in test_loader:\n",
    "    \n",
    "    sums = sums + len(v_inputs)\n",
    "    \n",
    "    v_inputs, v_labels = v_inputs.to(device), v_labels.to(device)\n",
    "\n",
    "    v_output, v_h = net(v_inputs)\n",
    "    \n",
    "    v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "        \n",
    "\n",
    "    output = torch.round(v_output.squeeze()).detach().cpu().numpy().astype(int)\n",
    "\n",
    "    ground = v_labels.detach().cpu().numpy().astype(int)\n",
    "\n",
    "    count = count + np.sum(output == ground)\n",
    "    \n",
    "print(\"Accuracy: \" + str(count/len(test_x)))\n",
    "print(len(test_x))\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/taco/lib/python3.7/site-packages/ipykernel_launcher.py:152: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative:\tIt make me happy\n",
      "negative:\tUnpleasant viewing experience\n",
      "postive:\tI am interested with this assigment\n",
      "negative:\tPoor you\n",
      "postive:\tHappy new year\n"
     ]
    }
   ],
   "source": [
    "def inference(net, review, seq_length = 200):\n",
    "    device = \"cuda\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    text = review.lower()\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])\n",
    "    words = text\n",
    "    \n",
    "    encoded_train =tokenizer.texts_to_sequences([words])\n",
    "    padded_words = pad_text(encoded_train, seq_length = 200)\n",
    "    padded_words = torch.from_numpy(padded_words).to(device)\n",
    "\n",
    "    \n",
    "    net.eval().to(device)\n",
    "    output, h = net(padded_words )#, h)\n",
    "    pred = torch.round(output.squeeze())  \n",
    "    return pred\n",
    "\n",
    "Test = [\n",
    "    \"It make me happy\",\n",
    "    \"Unpleasant viewing experience\",\n",
    "    \"I am interested with this assigment\",\n",
    "    \"Poor you\",\n",
    "    \"Happy new year\"\n",
    "]\n",
    "for t in Test:\n",
    "    lab = inference(net, t).tolist()\n",
    "    if int(lab) == 0:\n",
    "        print(\"negative:\\t\"+t)\n",
    "    else:\n",
    "        print(\"postive:\\t\"+t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taco",
   "language": "python",
   "name": "taco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
